{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05459032",
   "metadata": {},
   "source": [
    "# LSTM (장기 의존성 학습 모델)\n",
    "- RNN 구조에서 발생하는 문제를 해결하기 위해 구조를 변경한 모델 \n",
    "- 기존의 RNN은 학습의 횟수가 증가하면 과거의 기억이 잃는다. \n",
    "    - 반복 학습을 하면서 가중치의 변화량이 0에 가까워짐을 의미 \n",
    "    - 손실 함수 -> tanh -> 반복적으로 tanh() 함수를 실행하게 되면 변화량 0\n",
    "- LSTM은 반복 학습을 하면서 과거의 기억 중 장기 기억과, 단기 기억으로 나눠주는 역할\n",
    "    - 반복 학습을 하면서 tanh() 중복적으로 실행이 되면 기울기 변화량 줄어듬.\n",
    "    - 장기 기억은 tank() 함수를 실행하지 않고 -> 가중치의 변화를 계속 주겠다.\n",
    "    - 셀이라는 공간에 장기 기억 정보를 전달 \n",
    "    - 게이트로 정도를 선택적으로 보관, 삭제, 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d730ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd7afbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로드 \n",
    "df = pd.read_csv(\"../csv/AAPL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c578651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date 컬럼의 dtype을 시계열 데이터로 변환 \n",
    "# 문자형 데이터를 시계열 데이터로 변환 -> \n",
    "# 그래프 시각화할때 X축의 데이터로 사용하기 위함\n",
    "df['Date'] = pd.to_datetime(\n",
    "    df['Date']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a82eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 컬럼 선택 \n",
    "df = df[['Date', 'Adj Close', 'Volume']]\n",
    "# 결측치가 포함되어있는 행을 모두 제거 \n",
    "df.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f4233ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독립 데이터, 종속 데이터를 생성 \n",
    "X_all = df[['Adj Close', 'Volume']]\n",
    "Y_all = df[['Adj Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1444d659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7770, 2) (1943, 2)\n",
      "(7770, 1) (1943, 1)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 8:2 비율로 나눠준다. \n",
    "split_idx = int(len(df) * 0.8)\n",
    "X_train, X_test = X_all[ : split_idx], X_all[split_idx : ]\n",
    "Y_train, Y_test = Y_all[ : split_idx], Y_all[split_idx : ]\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ecc802d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 설정 값\n",
    "# 시계열 데이터의 구간(과거 데이터의 개수)\n",
    "window = 60\n",
    "# 학습 루프에서 루프의 수 \n",
    "epochs = 20\n",
    "# 학습 데이터의 배치의 개수\n",
    "x_batch = 64\n",
    "# optimizer lr 값 (학습율)\n",
    "lr = 0.001\n",
    "# 은닉 층의 뉴런의 개수 \n",
    "hidden_cnt = 64\n",
    "# 랜덤의 일반화\n",
    "torch.manual_seed(42)\n",
    "# 은닉층, 셀 사용 여부\n",
    "head_type = 'h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28343316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowDataset(Dataset):\n",
    "    # 입력 데이터와 타깃 데이터를 받아서 \n",
    "    # 구간의 수만큼 입력데이터를 잘라주고 \n",
    "    # 해당 구간 바로 다음 행의 타깃 데이터를 생성\n",
    "    # 구간의 데이터와 다음행의 타깃 데이터를 되돌려준다\n",
    "    # DataLoader 클래스에서 사용하기 위함\n",
    "    def __init__(\n",
    "            self, _x, _y, _window\n",
    "    ):\n",
    "        self.x = _x\n",
    "        self.y = _y\n",
    "        self.window = _window\n",
    "        # n의 값은 구간의 데이터 시작 지점의 최대값\n",
    "        self.n = len(_x) - _window\n",
    "    \n",
    "    def __len__(self):\n",
    "        # window 의 수치가 x의 길이보다 큰 경우에는 1을 되돌려준다. \n",
    "        return max(self.n, 1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # idx : 구간의 시점 지점 -> 최대 값은 self.n\n",
    "        # idx부터 idx+window-1 지점까지의 데이터 -> (window, 2)\n",
    "        x_window = self.x[idx : idx + self.window]\n",
    "        # idx+window 위치의 데이터  -> 단일값 생성\n",
    "        y_next = self.y[idx + self.window]\n",
    "        # tensor형의 변환 \n",
    "        x_tensor = torch.from_numpy(x_window)\n",
    "        y_tensor = torch.from_numpy(y_next)\n",
    "\n",
    "        return x_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ac05ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터들 스케일링 -> MinMaxScaler\n",
    "x_scaler = MinMaxScaler().fit(X_train)\n",
    "y_scaler = MinMaxScaler().fit(Y_train)\n",
    "\n",
    "X_train = x_scaler.transform(X_train)\n",
    "X_test = x_scaler.transform(X_test)\n",
    "Y_train = y_scaler.transform(Y_train)\n",
    "Y_test = y_scaler.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9417e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WindowDataset 생성 \n",
    "# 객체을 생성하고 getitem() 함수를 사용한다면 window(60) 구간의 데이터와\n",
    "# 해당 구간 다음 행의 종가 데이터를 되돌려주는 class\n",
    "train_ds = WindowDataset(X_train, Y_train, window)\n",
    "test_ds = WindowDataset(X_test, Y_test, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a6ad7362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader -> WindowDataset  class를 이용하여 \n",
    "# ds_data에서 len() 함수를 이용하여 해당 구간에서 가장 마지막에 사용 가능한 시작 지점\n",
    "\n",
    "train_dl = DataLoader(train_ds, shuffle=True, drop_last= True, \n",
    "                      batch_size=x_batch)\n",
    "test_dl = DataLoader(test_ds, shuffle=False, drop_last = False, \n",
    "                     batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "628e2fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n",
      "torch.Size([64, 60, 2])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dl:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bac1cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for x, y in test_dl:\n",
    "#     print(\"구간 데이터 : \",x)\n",
    "#     # print(\"다음 날의 종가 :\", y)\n",
    "#     if i == 2 : \n",
    "#         break\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d8056b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 모델 정의 \n",
    "\n",
    "class LSTMReg(nn.Module):\n",
    "    # h : 마지막의 은닉층의 데이터를 이용 (RNN -> 기억 소실 , LSTM -> 장기 적인 기억은 유지)\n",
    "    # c : 셀의 데이터를 이용 \n",
    "    # h_c : 은닉층 + 셀의 데이터를 이용\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_size, hidden_size, \n",
    "            num_layers = 1, dropout = 0.0, \n",
    "            bidirectional = False, head_type = 'h'\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # lstm 기본 설정 \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = input_size, \n",
    "            hidden_size= hidden_size, \n",
    "            num_layers= num_layers, \n",
    "            dropout= dropout, \n",
    "            bidirectional= bidirectional\n",
    "        )\n",
    "\n",
    "        # bidirectional이 True인 경우 차원의 개수 * 2\n",
    "        out_dim = hidden_size * (2 if bidirectional else 1)\n",
    "        # head_type은 은닉층만 사용하거나 셀만 사용 -> 차원의 개수 * 1 \n",
    "        if head_type in ['h', 'c']:\n",
    "            head_in = out_dim\n",
    "        elif head_type == 'h_c':\n",
    "            head_in  = out_dim * 2\n",
    "        else:\n",
    "            print('head_type Error')\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(head_in, 1)\n",
    "        )\n",
    "        self.head_type = head_type\n",
    "    \n",
    "    # 순전파 함수 \n",
    "    def forward(self, x):\n",
    "        # 순전파의 예측 결과 값\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        # 은닉층 중 마지막 은닉층을 저장 \n",
    "        h_last = h_n[-1]\n",
    "        # 셀 마지막 셀 \n",
    "        c_last = torch.tanh(c_n[-1])\n",
    "\n",
    "        if self.head_type == 'h':\n",
    "            feat = h_last\n",
    "        elif self.head_type == 'c':\n",
    "            feat = c_last\n",
    "        elif self.head_type == 'h_c':\n",
    "            feat = torch.cat( [h_last, c_last], dim = -1 )\n",
    "        \n",
    "        return self.head(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb695e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMReg(\n",
       "  (lstm): LSTM(2, 64)\n",
       "  (head): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMReg(input_size=2, hidden_size=hidden_cnt, \n",
    "                head_type=head_type)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f21ae87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 \n",
    "criterion = nn.MSELoss()\n",
    "# 옵티마이저 \n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "16e30c70",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (60) must match the size of tensor b (64) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m optimizer.zero_grad()\n\u001b[32m     26\u001b[39m yhat = model(x)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43myhat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m loss.backward()\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 기울기 폭발 방지 -> nan값이 나올수 있는 확률은 배제\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:616\u001b[39m, in \u001b[36mMSELoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3868\u001b[39m, in \u001b[36mmse_loss\u001b[39m\u001b[34m(input, target, size_average, reduce, reduction, weight)\u001b[39m\n\u001b[32m   3865\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3866\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3868\u001b[39m expanded_input, expanded_target = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3870\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m weight.size() != \u001b[38;5;28minput\u001b[39m.size():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\functional.py:77\u001b[39m, in \u001b[36mbroadcast_tensors\u001b[39m\u001b[34m(*tensors)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, *tensors)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (60) must match the size of tensor b (64) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "\n",
    "# 평가 함수 생성 \n",
    "# 함수 호출시 동시에 실행이 되는 데코레이터 생성 \n",
    "@torch.no_grad()\n",
    "def evaluate_mse(dataloader):\n",
    "    model.eval()\n",
    "    total_loss, total_n = 0.0, 0\n",
    "    # with torch.no_grad():\n",
    "    for x, y in dataloader:\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "        yhat = model(x)\n",
    "        loss = criterion(yhat, y)\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        total_n += y.size(0)\n",
    "    return total_loss / max(total_n, 1)\n",
    "\n",
    "train_history, val_history = [], []\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    running, n_seen = 0.0, 0\n",
    "    for x, y in train_dl:\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(x)\n",
    "        loss = criterion(yhat, y)\n",
    "        loss.backward()\n",
    "\n",
    "        # 기울기 폭발 방지 -> nan값이 나올수 있는 확률은 배제\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running += loss.item() * y.size(0)\n",
    "        n_seen += y.size(0)\n",
    "    train_mse = running / max(n_seen, 1)\n",
    "    val_mse = evaluate_mse(test_dl)\n",
    "    train_history.append(train_mse)\n",
    "    val_history.append(val_mse)\n",
    "    print(f\"Epoch : {epoch+1}, train_mse : {round(train_mse, 4)}, val_mse : {round(val_mse, 4)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
