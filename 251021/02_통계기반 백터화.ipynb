{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0442f3d5",
   "metadata": {},
   "source": [
    "# 통계기반 자연어 처리 \n",
    "\n",
    "## NLP의 접근 방식 \n",
    "1. 규칙 기반 \n",
    "    - 사람의 규칙(문법 / 패턴)을 정의\n",
    "2. 통계 기반\n",
    "    - 단어의 빈도, 확률, 통계를 활용\n",
    "3. 딥러닝 기반 \n",
    "    - 대규모 데이터 + 신경망 모델 활용\n",
    "\n",
    "## N-gram 근사 \n",
    "- Unigram \n",
    "    - N = 1 \n",
    "    - 독립 단어 (하나의 단어)\n",
    "- Bigram\n",
    "    - N = 2\n",
    "    - 바로 앞의 단어를 고려 \n",
    "- Trigram\n",
    "    - N = 3\n",
    "    - 앞의 2개의 단어를 고려 \n",
    "\n",
    "## 로그 확률\n",
    "- 단어의 조건부 확률은 매우 작은 값 (0.0001, 0.00001)\n",
    "- 조건부의 확률들 끼리 곱하게 되면 -> 0에 가까운 값 -> 계산이 불안정 \n",
    "- 이러한 문제를 해결하기 위해 log 값을 사용 \n",
    "\n",
    "## 혼란도(Preplexity)\n",
    "- 문장을 얼마나 햇갈려하는지를 수치로 표현한 값\n",
    "- 값이 높다면 -> 문장 이해도가 내려간다.\n",
    "- 값이 낮다면 -> 문장 이해도가 올라간다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24be7f1c",
   "metadata": {},
   "source": [
    "### BoW \n",
    "- 문서의 단어 순서는 무시 \n",
    "- 각 문서에서 단어가 몇번 등장했는가?(빈도수)\n",
    "- 가장 기본적인 백터화 방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77edc096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 데이터셋 생성 \n",
    "docs = [\n",
    "    \"영화가 정말 재미있었다\", \n",
    "    \"영화가 너무 지루하다\", \n",
    "    \"배우의 연기가 너무 좋았다.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca4b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [doc.split() for doc in docs]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e1a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens에서 각각의 단어들을 하나의 리스트로 생성하고 중복은 제거 \n",
    "# tokens의 2차원 리스트를 1차원으로 변환 \n",
    "vocab1 = []\n",
    "for token in tokens:\n",
    "    # token -> tokens의 각 원소들 -> 1차원 리스트 \n",
    "    for word in token:\n",
    "        # word : token이라는 1차원 리스트의 각각의 원소들 \n",
    "        vocab1.append(word)\n",
    "vocab1 = list(set(vocab1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f17d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(sum(tokens, [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547f93ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bow 행렬을 하나 생성 \n",
    "# tokens의 문장에 vocab의 단어가 몇개 포함되어있는가?\n",
    "bow_list = []\n",
    "vocab.sort()\n",
    "for doc in tokens:\n",
    "    # row = [ doc.count(word) for word in vocab ]\n",
    "    row = []\n",
    "    for word in vocab:\n",
    "        # count() 함수는 list에서 인자의 값과 같은 데이터가 몇개 있는가?\n",
    "        row.append(doc.count(word))\n",
    "    bow_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1919780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d1b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow = pd.DataFrame(bow_list, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd05b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7192087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b93bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectorizer = pd.DataFrame(X.toarray(), \n",
    "    columns = vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5efe342",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b186a230",
   "metadata": {},
   "source": [
    "### TF-IDF \n",
    "- Bow 모델(단순 빈도 모델)의 한계를 보완하기 위한 통계 기반 기법\n",
    "\n",
    "- TF\n",
    "    - 문서(문장들의 집합) 내의 빈도가 높을수록 값이 큼\n",
    "    - 자주 등장 할수록 그 문서에서 중요하다고 판단 \n",
    "- IDF\n",
    "    - 문장 내에서의 단어의 희소성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b6107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전의 길이 \n",
    "V = len(vocab)\n",
    "# 전체 문서의 길이 \n",
    "N = len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어의 개수를 생성 \n",
    "word_cnt = {\n",
    "    w : sum(1 for doc in tokens if w in doc) for w in vocab\n",
    "}\n",
    "word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e0b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF 계산식 함수 \n",
    "def tf(word, doc):\n",
    "    # word : 단어 사전의 각 원소\n",
    "    # doc : tokens 각 원소 \n",
    "    # doc.count(word) : 문장에서 특정 단어의 개수 \n",
    "    # len(doc) : 문장의 단어의 개수\n",
    "    result = doc.count(word) / len(doc)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b892769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF 계산식 함수 \n",
    "def idf(word):\n",
    "    # word : 단어 사전의 각 원소\n",
    "    # N : docs의 길이 -> 문장들의 개수 \n",
    "    # word_cnt[word] -> 문서에서 특정 단어의 개수\n",
    "    result = math.log( (N) / (word_cnt[word] + 1) ) + 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eaaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF -> TF의 값과 IDF 값을 곱한 수치 \n",
    "X_tfidf = [ \n",
    "    [tf(w, doc) * idf(w) for w in vocab] for doc in tokens \n",
    "]\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_tfidf, columns = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bfa9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF는 scikit-learn에 class가 존재 \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab7268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = TfidfVectorizer(\n",
    "    ngram_range= (1,1), \n",
    "    min_df=1\n",
    ")\n",
    "X =  vector.fit_transform(docs)\n",
    "pd.DataFrame(\n",
    "    X.toarray(), \n",
    "    columns = vector.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d1cc98",
   "metadata": {},
   "source": [
    "- 감정 분석에서 TF-IDF + SVM의 조합이 보편적으로 사용\n",
    "    - 단어별로 feature가 생성이 되기 때문에 고차원 데이터가 생성이 되고 value가 0이 데이터가 많은 비율을 차지 하기 때문에 트리 구조를 중복으로 사용하는 배깅 부스팅은 어울리지 않는다\n",
    "- ngram_range는 (1,2)로 사용을 하게 되면 '너무 좋았다', '정말 별로다' 이러한 단어 패턴도 포착 가능 \n",
    "- 고차원으로 생성이 된 행렬 데이터를 차원 축소 기법을 이용하여 차원의 수를 줄이고 모델에 학습하여 성능을 평가하는 방법도 존재 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e484bcf",
   "metadata": {},
   "source": [
    "### N-gram\n",
    "- 단어를 몇 개까지 하나로 볼것인가?\n",
    "- 언어의 모델을 문장의 자연스러움을 수치화하는 통계적 방법 \n",
    "- 수치화 값으로는 로그확률, 혼란도 \n",
    "- 로그확률은 매우 크지 않은 음수인 경우 자연스러움 표현 \n",
    "- 혼란도 낮을수록 자연스러움을 표현  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 데이터를 생성 \n",
    "texts = [\n",
    "    '오늘 날씨가 좋다', \n",
    "    '오늘 기분이 좋다', \n",
    "    '내일 날씨가 맑다', \n",
    "    '기분이 아주 좋다', \n",
    "    '날씨가 아주 좋다'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a291c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b779f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 함수 생성 \n",
    "okt = Okt()\n",
    "def tokenize(text):\n",
    "    result = okt.morphs(text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369e2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장의 시작과 끝 부분에 태그를 추가 \n",
    "tokens = [ ['<s>'] + tokenize(text) + ['</s>'] for text in texts ]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d4b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram, bigram 카운트 확인 \n",
    "from collections import Counter\n",
    "unigram = Counter(w for token in tokens for w in token)\n",
    "bigram = Counter(( token[i], token[i+1] ) for token in \\\n",
    "                 tokens for i in range(len(token) - 1) )\n",
    "print(unigram)\n",
    "print(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f25d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 다중 for문을 풀어서 작성 \n",
    "# for token in tokens:\n",
    "#     for i in range(len(token) - 1):\n",
    "#         print(token[i], token[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14125dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 내에서 단어의 개수 \n",
    "V = len(unigram)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확률이 0이 나오는 경우\n",
    "# Add-1 Smoothing(Laplace Smoothing) \n",
    "# 분자에 + 1 -> 분모에는 + 문장에서의 단어의 개수  \n",
    "def bigram_prob(prev, curr):\n",
    "    # bigram[(prev, curr)] -> 연결된 두개의 단어의 개수\n",
    "    # unigram[prev] -> 앞의 단어의 개수\n",
    "    # 특정 단어 뒤에 단어가 나올 확률\n",
    "    result = (bigram[(prev, curr)] + 1) / ( unigram[prev] + V)\n",
    "    return result\n",
    "\n",
    "# 로그 확률 -> 음수의 절대값이 크지 않다면 자연스러운 구조 \n",
    "def log_prob(tokens):\n",
    "    # tokens : 문장을 토큰화한 리스트\n",
    "    # 문장의 시작과 끝에 태그 추가 \n",
    "    seq = ['<s>'] + tokens + ['</s>']\n",
    "    # 로그 확률의 누적합을 위해 초기 값 0.0 지정 \n",
    "    log_p = 0.0\n",
    "    for i in range(len(seq) - 1):\n",
    "        # 확률 생성\n",
    "        p = bigram_prob(seq[i], seq[i+1])\n",
    "        # log_p에 누적합 \n",
    "        log_p += math.log(p)\n",
    "    return log_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c56b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혼란도 \n",
    "def perplexity(tokens):\n",
    "    # tokens : 문장을 토큰화 한 리스트\n",
    "    log_p = log_prob(tokens)\n",
    "    T = len(tokens) + 1\n",
    "    # 자연상수 계산식\n",
    "    result = math.exp(-log_p / T)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f8ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 unigram / bigram을 이용하여 확률을 계산하고 로그 확률, \n",
    "# 혼란도 확인 \n",
    "new_texts = [\n",
    "    '오늘 날씨가 맑다', \n",
    "    '기분이 내일 좋다', \n",
    "    '날씨가 아주 좋다'\n",
    "]\n",
    "\n",
    "result = []\n",
    "for text in new_texts:\n",
    "    tokens = tokenize(text)\n",
    "    # 로그확률\n",
    "    lp = log_prob(tokens)\n",
    "    # 혼란도 \n",
    "    perple = perplexity(tokens)\n",
    "    result.append([text, lp, perple])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b922d9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장</th>\n",
       "      <th>로그확률</th>\n",
       "      <th>혼란도</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>오늘 날씨가 맑다</td>\n",
       "      <td>-8.536211</td>\n",
       "      <td>5.513735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>기분이 내일 좋다</td>\n",
       "      <td>-9.694247</td>\n",
       "      <td>6.950749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>날씨가 아주 좋다</td>\n",
       "      <td>-7.843064</td>\n",
       "      <td>4.799985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          문장      로그확률       혼란도\n",
       "0  오늘 날씨가 맑다 -8.536211  5.513735\n",
       "1  기분이 내일 좋다 -9.694247  6.950749\n",
       "2  날씨가 아주 좋다 -7.843064  4.799985"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result, columns = ['문장', '로그확률', '혼란도'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e4080d",
   "metadata": {},
   "source": [
    "## 실습 문제 \n",
    "- ratings_test.txt 파일을 로드 \n",
    "- document 컬럼의 결측치를 제외하고 list 형태로 변환(데이터의 개수는 상위 5000개만)\n",
    "- unigram, bigram 단어의 개수를 생성 \n",
    "- 로그 확률, 혼란도 함수를 생성\n",
    "- new_texts를 이용하여 로그확률, 혼란도를 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e90a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = [\n",
    "    '이 영화 좋네', \n",
    "    '시간이 가는줄 몰랐다', \n",
    "    '아 이런 내 시간', \n",
    "    '연기가 그닥 ... 배우 ... '\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
