{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa58130b",
   "metadata": {},
   "source": [
    "### FastText \n",
    "- Word2Vec에서 OOV(사전에 없는 용어) 문제를 해결하기 위한 모델 \n",
    "- Word2Vec에서는 '강아지' 와 '강아지들' 문구를 다른 단어로 생각\n",
    "- FastText는 Word2Vec의 학습 방식은 비슷 \n",
    "    - 단어를 한글씩 잘게 쪼개서 단어의 유사도를 생성 \n",
    "    - '강아지' -> '강', '아', '지', '강아', '아지', '강아지'(n-gram 방식)\n",
    "    - word2vec과 기본적인 매개변수는 같지만 min_n, max_n 매개변수가 존재 \n",
    "    - subword의 최소 길이와 최대 길이를 설정 \n",
    "    - min_n = 1, max_n =1 --> subword를 사용하지 않겠다 -> Word2Vec 같은 형태로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c017b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb67d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 문장 생성\n",
    "sentences = [\n",
    "    ['이커머스', '데이터', '분석', '진행'], \n",
    "    ['상품', '리뷰', '기반', '감성', '분석', '합니다'], \n",
    "    ['형태소', '단위', '임베딩', '가능']\n",
    "]\n",
    "# Fast Text 모델에 학습 \n",
    "model  = FastText(\n",
    "    sentences=sentences, \n",
    "    vector_size= 50,        # 단위 벡터의 차원 수\n",
    "    window=3,               # 주변의 확인할 단어의 개수\n",
    "    min_count=1,            # 최소 출현 횟수\n",
    "    sg = 1,                 # Skip-gram 방식으로 확률 계산\n",
    "    epochs= 10,             # 학습 반복 횟수\n",
    "    min_n = 2, \n",
    "    max_n= 6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c4704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 단어의 벡터를 확인 \n",
    "model.wv['이커머스']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c8d1125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('이커머스', 0.17707844078540802),\n",
       " ('합니다', 0.17073467373847961),\n",
       " ('가능', 0.16921374201774597)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 유사 단어를 확인 \n",
    "model.wv.most_similar('데이터', topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42637dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 내용에 없는 단어를 확인 \n",
    "model.wv['감정']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b7f0c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec을 이용해서 sentences 학습하고 없는 단어 확인 \n",
    "model2 = Word2Vec(\n",
    "    sentences=sentences, \n",
    "    window=3, \n",
    "    vector_size=50, \n",
    "    min_count=1, \n",
    "    sg = 1, \n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "464d52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 내용에 없는 단어를 출력 \n",
    "# model2.wv['감정']\n",
    "# Word2Vec은 사전에 없는 단어를 단위벡터로 확인하면 에러가 발생 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d30143b",
   "metadata": {},
   "source": [
    "- Word2Vec과 FastText와 단어간의 유사도 차이 확인 \n",
    "\n",
    "    - '강아지', '강아지들' 두개의 단어를 유사한 단어\n",
    "        - Word2Vec은 다른 단어로 인식 -> 유사도 낮게 \n",
    "        - FastText는 비슷한 단어로 인식 -> 유사도 높게 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f6c6916",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences2 = [\n",
    "    [\"고양이\", \"고양이들\", '귀엽다', '동물', '반려동물'], \n",
    "    ['강아지', '강아지들', '귀엽다', '동물', '반려동물'], \n",
    "    ['달리다', '달리는', '달림', '걷다', '걷는'], \n",
    "    ['빠르다', '빠른', '느리다', '느림'], \n",
    "    ['예쁘다', '예쁨', '예쁜', '매력적이다'], \n",
    "    ['컴퓨터', '컴퓨팅', '컴퓨터들', '기계']\n",
    "]\n",
    "\n",
    "# 모델학습 (word2vec, fasttext)\n",
    "w2v = Word2Vec(\n",
    "    sentences=sentences2, \n",
    "    vector_size=100,\n",
    "    window = 3, \n",
    "    min_count=1, \n",
    "    sg =1, \n",
    "    epochs=50, \n",
    "    seed = 42\n",
    "    )\n",
    "ft = FastText(\n",
    "    sentences=sentences2, \n",
    "    vector_size= 100,\n",
    "    window = 3, \n",
    "    min_count=1, \n",
    "    sg = 1, \n",
    "    epochs=50, \n",
    "    seed=42, \n",
    "    min_n= 2, \n",
    "    max_n=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb6aa861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec 유사도 0.20342888\n",
      "FastText 유사도 0.385201\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec의 유사도 확인 \n",
    "print(\n",
    "    'Word2Vec 유사도', w2v.wv.similarity('강아지', '강아지들')\n",
    ")\n",
    "# FastText의 유사도 확인 \n",
    "print(\n",
    "    'FastText 유사도', ft.wv.similarity('강아지', '강아지들')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aafe605f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "강아지 - 강아지들의 유사도 : Word2Vec (0.2034)           FastText (0.3852)\n",
      "고양이 - 고양이들의 유사도 : Word2Vec (-0.1262)           FastText (0.3965)\n",
      "달리다 - 달리는의 유사도 : Word2Vec (-0.0706)           FastText (0.2667)\n",
      "예쁘다 - 예쁜의 유사도 : Word2Vec (0.1392)           FastText (0.0696)\n",
      "컴퓨터 - 컴퓨팅의 유사도 : Word2Vec (-0.0516)           FastText (0.3647)\n",
      "빠르다 - 느림의 유사도 : Word2Vec (0.0108)           FastText (-0.0127)\n"
     ]
    }
   ],
   "source": [
    "# 비교 대상 단어들 \n",
    "test_text = [\n",
    "    ['강아지', '강아지들'], \n",
    "    ['고양이', '고양이들'], \n",
    "    ['달리다', '달리는'], \n",
    "    ['예쁘다', '예쁜'], \n",
    "    ['컴퓨터', '컴퓨팅'], \n",
    "    ['빠르다', '느림']\n",
    "]\n",
    "\n",
    "for a, b in test_text:\n",
    "    w2v_sim = float(w2v.wv.similarity(a, b))    # 유사도 데이터를 실수형 변경\n",
    "    ft_sim = float(ft.wv.similarity(a, b))      # 유사도 데이터를 실수형 변경\n",
    "    print(f\"{a} - {b}의 유사도 : Word2Vec ({round(w2v_sim, 4)}) \\\n",
    "          FastText ({round(ft_sim, 4)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f61f9801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['무선', '이어폰', '블루투스', '노이즈캔슬링', '충전케이스'],\n",
       " ['유선', '이어폰', '하이파이', '금도금', '플러그'],\n",
       " ['게이밍', '마우스', 'RGB', '경량', '디자인'],\n",
       " ['무선', '마우스', '초경량', '블루투스', '듀얼모드'],\n",
       " ['헤드폰', '노이즈캔슬링', '유선']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상품 명을 기준으로 특정 상품을 검색 시 연관 된 상품의 목록을 확인 \n",
    "\n",
    "products = {\n",
    "    'P001' : '무선 이어폰 블루투스 노이즈캔슬링 충전케이스', \n",
    "    'P002' : '유선 이어폰 하이파이 금도금 플러그', \n",
    "    'P003' : '게이밍 마우스 RGB 경량 디자인', \n",
    "    'P004' : '무선 마우스 초경량 블루투스 듀얼모드', \n",
    "    'P005' : '헤드폰 노이즈캔슬링 유선'\n",
    "}\n",
    "# products에서 FastText 통해 학습을 시키기 위해 데이터를 추출 \n",
    "# 필요한 데이터는 dict형 데이터에서 values\n",
    "datas = products.values()\n",
    "# datas을 공백을 기준으로 나눠준다. \n",
    "tokens = []\n",
    "for data in datas:\n",
    "    tokens.append( data.split() )\n",
    "\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bda2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 된 데이터를 이용하여 FastText에 학습 \n",
    "ft2 = FastText(\n",
    "    sentences= tokens, \n",
    "    vector_size= 100, \n",
    "    window = 3, \n",
    "    min_count=1, \n",
    "    sg = 1, \n",
    "    epochs= 10, \n",
    "    min_n = 3, \n",
    "    max_n = 6, \n",
    "    seed = 42\n",
    ")   # subword를 사용하는 FastText 모델에 학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e21d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단위 벡터 확인  -> vector_size 차원의 좌표\n",
    "ft2.wv['마우스']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "876866a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 문장의 평균 벡터를 구하는 함수 정의 \n",
    "def sent_vec(token, type = None):\n",
    "    vecs = []\n",
    "    for w in token:\n",
    "        vecs.append(ft.wv[w])\n",
    "    # 해당 vecs가 존재하지 않는다면 희소행렬을 되돌려준다. \n",
    "    if not vecs:\n",
    "        return np.zeros(ft.vector_size)\n",
    "    print(np.array(vecs).shape)\n",
    "    v = np.mean(vecs, axis=0)\n",
    "    # 일반적인 문장의 평균 백터를 구하는 식 \n",
    "    # 성능을 올리기 위해서는 L2 정규화 -> 벡터의 거리로 나눠준다. \n",
    "    if type == 'l2':\n",
    "        v = v / (np.linalg.norm(v) + 1e-12)     \n",
    "        # 1e-12을 더한 이유는 0으로 나눠지는것을 방지 하기 위함\n",
    "    print(v.shape)\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44fec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 된 데이터들을 평균 벡터로 변환 \n",
    "item_vecs = []\n",
    "for t in tokens :\n",
    "    # print(sent_vec(t))\n",
    "    # break\n",
    "    item_vecs.append(sent_vec(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9de088ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균 벡터를 이용해서 코사인 유사도를 확인 하기 위해서 \n",
    "# sklearn  코사인 유사도 함수를 로드 \n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96a5536b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08545259, 0.09662677, 1.0000001 , 0.21450484, 0.09070536]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = list(products.keys()).index('P003')\n",
    "cosine_similarity(item_vecs[idx:idx+1], item_vecs)\n",
    "\n",
    "cosine_similarity([item_vecs[idx]], item_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5e3e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도를 이용해서 가장 근접한 상품의 이름을 출력 \n",
    "# 추천 하는 함수를 생성 \n",
    "def recommand_by_text(product_id):\n",
    "    # 해당 상품명의 위치 값 -> \n",
    "    # item_vecs위치를 이용하여 코사인 유사도를 생성하기 위함\n",
    "    # 상품의 id값들은 products라는 dict에서 해당 id의 위치를 저장 \n",
    "    idx = list(products.keys()).index(product_id)\n",
    "\n",
    "    # item_vecs에서 해당 인덱스의 값과 전체 vectors의 값을 비교\n",
    "    # 코사인 유사도를 확인하면 하나의 문장을 유사도를 확인했기 때문에 2차원이 아닌 \n",
    "    # 1차원 데이터를 생성 \n",
    "    sims = cosine_similarity(item_vecs[idx:idx+1], item_vecs).ravel()\n",
    "    # 내림차순 정렬 \n",
    "    order = sims.argsort()[::-1]\n",
    "    # 추천 단어들을 출력 \n",
    "    rec = []\n",
    "    for i in order:\n",
    "        # 코사인 유사도에서 같은 id 라면 추천하지 않는다. \n",
    "        if list(products.keys())[i] != product_id:\n",
    "            # 상품의 id와 유사도의 값들을 rec 추가\n",
    "            rec.append([ list(products.keys())[i], sims[i] ])\n",
    "    return rec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0533040",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_list = recommand_by_text('P001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8dfabf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "무선 마우스 초경량 블루투스 듀얼모드\n",
      "유선 이어폰 하이파이 금도금 플러그\n"
     ]
    }
   ],
   "source": [
    "for idx, (pid, _) in enumerate(rec_list):\n",
    "    # 유사도가 높은 상위 2개 상품의 이름을 확인 \n",
    "    print(products[pid])\n",
    "    if idx == 1:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
