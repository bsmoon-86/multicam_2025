{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb871b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6446f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"../data/인터뷰/ckmk_d_bm_f_n_169087.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f8468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5554b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_json을 통해 json파일을 로드했는데? 데이터 형태가 데이터프레임에 최적화x\n",
    "# 답변의 데이터, 답변의 요약데이터를 추출 -> kobart 모델을 이용해서 요약 학습\n",
    "# json 파일을 dict 형태로 변환 -> 데이터 추출\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4b290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일을 로드해서 dict 형태로 변환 \n",
    "with open(\"../data/인터뷰/ckmk_d_bm_f_n_169087.json\", 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0531cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = open(\"../data/인터뷰/ckmk_d_bm_f_n_169087.json\", 'r', encoding='utf-8')\n",
    "data2 = json.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40775bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f52ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c1d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60323582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataSet': {'answer': {'emotion': [],\n",
      "                        'intent': [{'category': 'attitude',\n",
      "                                    'expression': '',\n",
      "                                    'text': ''}],\n",
      "                        'raw': {'text': '저는 저 혼자 시간을 보내는 것을 우선적으로 하고요. 예를 들어서 '\n",
      "                                        '혼자 산책을 한다든지 아니면 혼자 카페를 간다든지 아니면 혼자 코인 '\n",
      "                                        '노래방을 간다든지 해서 스트레스를 해소하는 편입니다. 물론 남들과 '\n",
      "                                        '만나면서 시간을 보내는 것도 굉장히 좋아하지만요. 아무래도 내 '\n",
      "                                        '혼자만의 시간이 있어야 어느 정도 생각 정리도 되고 그러면서 내 '\n",
      "                                        '안에 있던 응어리도 절로 풀러지는 그런 것을 경험하게 되었습니다. '\n",
      "                                        '그리고 코인 노래방이나 이런 곳에 가면은 아무래도 제가 노래 부르는 '\n",
      "                                        '것도 좋아하고 그리고 노래를 부르면서 그런 소리가 웅웅거리고 또 '\n",
      "                                        '소리를 지른다는 거에 대해서 해소감도 있는 것 같고요. 그래서 저는 '\n",
      "                                        '일단 혼자만의 시간을 보내는데 뭐 혼자서 노래방을 가거나 아니면 '\n",
      "                                        '혼자서 카페를 가거나 그런 식으로 스트레스를 해소한다 라고 말씀을 '\n",
      "                                        '드릴 수 있을 것 같습니다.',\n",
      "                                'wordCount': 107},\n",
      "                        'summary': {'text': '저는 혼자 시간을 보내는 것을 우선적으로 하며, 혼자 산책을 '\n",
      "                                            '하거나 혼자 카페를 간다든지 또는 혼자 노래방을 간다든지 '\n",
      "                                            '해서 스트레스를 해소하는 편입니다. 혼자의 시간이 있으면 '\n",
      "                                            '생각 정리도 되고 제 안에 있던 응어리도 저절로 풀러지는 '\n",
      "                                            '것을 경험하게 되었습니다.',\n",
      "                                    'wordCount': 36}},\n",
      "             'info': {'ageRange': '-34',\n",
      "                      'channel': 'MOCK',\n",
      "                      'date': '20230116',\n",
      "                      'experience': 'NEW',\n",
      "                      'gender': 'FEMALE',\n",
      "                      'occupation': 'BM',\n",
      "                      'place': 'ONLINE'},\n",
      "             'question': {'emotion': [],\n",
      "                          'intent': [],\n",
      "                          'raw': {'text': '지원자님께서 가지고 계신 나만의 스트레스 해소법이 있으신지 '\n",
      "                                          '있으시다고 한다면 왜 그러한 방법으로 스트레스를 해소하려고 '\n",
      "                                          '하셨는지 이야기해 주실 수 있으실까요',\n",
      "                                  'wordCount': 19}}},\n",
      " 'rawDataInfo': {'answer': {'audioPath': '/Mock/01.Management/Female/New/ckmk_a_bm_f_n_169087.wav',\n",
      "                            'channelCount': 1,\n",
      "                            'duration': 59380,\n",
      "                            'fileFormat': 'wav',\n",
      "                            'fileSize': 1900238,\n",
      "                            'samplingBit': 16,\n",
      "                            'samplingRate': '16kHz'},\n",
      "                 'question': {'audioPath': '/Mock/01.Management/Female/New/ckmk_q_bm_f_n_169087.wav',\n",
      "                              'channelCount': 1,\n",
      "                              'duration': 20760,\n",
      "                              'fileFormat': 'wav',\n",
      "                              'fileSize': 664398,\n",
      "                              'samplingBit': 16,\n",
      "                              'samplingRate': '16kHz'}},\n",
      " 'version': '1.0'}\n"
     ]
    }
   ],
   "source": [
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96016e4d",
   "metadata": {},
   "source": [
    "## kobart 복습 \n",
    "1. 인터뷰 폴더 안에 있는 json 파일들을 로드 \n",
    "    - 답변에 대한 데이터들을 하나의 리스트로 생성 \n",
    "    - 답변 요약에 대한 데이터들을 하나의 리스트로 생성 \n",
    "2. 위에서 생성된 데이터들중 train데이터는 처음부터 100번째 데이터까지 \n",
    "3. 마지막에서 10번째부터 마지막까지의 데이터를 validation 데이터로 사용\n",
    "4. train, test 데이터를 DatasetDict 형태로 변환 \n",
    "5. tokenizer와 model을 로드하여 데이터를 학습하고 검증 \n",
    "    - input의 최대 사이즈 : 512\n",
    "    - outpit의 최대 사이즈 : 256\n",
    "6. test 데이터를 생성 : 원본의 리스트에서 200번째의 데이터를 이용하여 요약을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79b080e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json 파일의 목록을 생성 \n",
    "import os \n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "562fa12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/인터뷰/\"\n",
    "file_list = os.listdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6da98830",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list2 = glob(\"../data/인터뷰/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1ac4e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/인터뷰/ckmk_d_bm_f_n_169087.json\n"
     ]
    }
   ],
   "source": [
    "for file in file_list:\n",
    "    # os 라이브러리를 이용하여 파일의 목록을 불러왔을때\n",
    "    path = file_path + file\n",
    "    print(path)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bff633e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid control character at: line 23 column 135 (char 755)\n",
      "Invalid control character at: line 23 column 571 (char 1132)\n",
      "Invalid control character at: line 23 column 433 (char 1029)\n"
     ]
    }
   ],
   "source": [
    "# answer의 원문과 요약 텍스트들을 저장할수 있는 빈 리스트 생성\n",
    "answers, summarys = [], []\n",
    "for file in file_list2:\n",
    "    # print(file)\n",
    "    # break\n",
    "    try:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            dict_data = json.load(f)\n",
    "            answer = dict_data['dataSet']['answer']['raw']['text']\n",
    "            summary = dict_data['dataSet']['answer']['summary']['text']\n",
    "            answers.append(answer)\n",
    "            summarys.append(summary)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c731acc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12801"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ec55441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 걸리는 시간이 길기때문에 일부분만 확인\n",
    "train_docs = answers[ : 100]\n",
    "train_sums = summarys[ : 100]\n",
    "valid_docs = answers[-10 : ]\n",
    "valid_sums = summarys[-10 : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86f9d92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "10 10\n"
     ]
    }
   ],
   "source": [
    "print( len(train_docs), len(train_sums) )\n",
    "print( len(valid_docs), len(valid_sums) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "143ca916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9822d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gogamza/kobart-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81963cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatasetDict 생성 \n",
    "raw_ds = DatasetDict(\n",
    "    {\n",
    "        'train' : Dataset.from_dict(\n",
    "            {\n",
    "                'document' : train_docs, \n",
    "                'summary' : train_sums\n",
    "            }\n",
    "        ), \n",
    "        'validation' : Dataset.from_dict(\n",
    "            {\n",
    "                'document' : valid_docs, \n",
    "                'summary' : valid_sums\n",
    "            }\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "926b74f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저, 모델 로드 \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast = True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# 입 출력 길이 설정 \n",
    "max_input_len = 512\n",
    "max_target_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb217dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_fn(batch):\n",
    "    inputs = tokenizer(\n",
    "        batch['document'], \n",
    "        max_length = max_input_len, \n",
    "        padding = 'max_length', \n",
    "        truncation = True\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch['summary'], \n",
    "            max_length = max_target_len, \n",
    "            padding = 'max_length', \n",
    "            truncation = True\n",
    "        )\n",
    "    # labels의 input_ids에서 padding_token을 -100으로 변경 -> inputs['labels'] 대입\n",
    "    labels_ids = np.array(labels['input_ids'])\n",
    "    labels_ids[labels_ids == tokenizer.pad_token_id ] = -100\n",
    "    inputs['labels'] = labels_ids.tolist()\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05ee256f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/100 [00:00<?, ? examples/s]c:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1460.70 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 1298.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds = raw_ds.map(\n",
    "    tok_fn, batched= True, remove_columns= ['document', 'summary']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49a57c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer= tokenizer, \n",
    "    model = model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "193f3910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics 지정 \n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90099761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    labels = np.where(\n",
    "        labels != -100, labels, tokenizer.pad_token_id\n",
    "    )\n",
    "    # 텍스트 디코딩 \n",
    "    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # 문장에서 좌우의 공백이 존재하는 경우 다른 값으로 측정하기때문에 좌우 공백 제거 \n",
    "    pred_str = [ doc.strip() for doc in pred_str ]\n",
    "    label_str = [ doc.strip() for doc in label_str ]\n",
    "\n",
    "    # ROUGE 계산 \n",
    "    result = rouge.compute(\n",
    "        predictions= pred_str, \n",
    "        references= label_str, \n",
    "        use_stemmer = True       # 점수 계산시 단어의 어간을 기준으로 비교하는 의미\n",
    "    )\n",
    "\n",
    "    # ROUGE를 보기 편한 형태로 변경 \n",
    "    result = { k : round(v * 100, 2) for k, v in result.items() }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c19b2473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 파라미터 값을 지정 \n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir= \"./kobart\", \n",
    "    eval_strategy= 'epoch', \n",
    "    save_strategy= 'epoch', \n",
    "    learning_rate= 5e-5, \n",
    "    num_train_epochs= 1, \n",
    "    logging_steps=10, \n",
    "\n",
    "    # generate 설정을 변경 \n",
    "    predict_with_generate=True, \n",
    "    generation_max_length= 70, \n",
    "    # 요약 데이터를 생성할때 문장 후보의 탐색의 개수를 설정\n",
    "    generation_num_beams= 4, \n",
    "\n",
    "    load_best_model_at_end= True, \n",
    "    metric_for_best_model='rougeL', \n",
    "    greater_is_better=True, \n",
    "    report_to=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b94cf5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekfla\\AppData\\Local\\Temp\\ipykernel_15064\\780977535.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "c:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 01:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.262100</td>\n",
       "      <td>1.652317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'forced_eos_token_id': 1}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13, training_loss=2.931919978215144, metrics={'train_runtime': 108.8611, 'train_samples_per_second': 0.919, 'train_steps_per_second': 0.119, 'total_flos': 30486822912000.0, 'train_loss': 2.931919978215144, 'epoch': 1.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer 생성 \n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model, \n",
    "    args=args, \n",
    "    train_dataset= tokenized_ds['train'], \n",
    "    eval_dataset=tokenized_ds['validation'], \n",
    "    tokenizer = tokenizer, \n",
    "    data_collator= data_collator, \n",
    "    compute_metrics= compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5e0be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = answers[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3c04bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sum = summarys[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd70b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저는 지금 백지연이 지은 크리티컬 매스 라는 책을 요즘 읽고 있는데요. 이 책을 읽으면서 제 장점을 또 끌어내고 그것을 할 수 있는 능력이 굉장히 뛰어나시잖아요. 그래서 타인을 이해할 수 있는 폭이 넓지 않을까 라는 생각에서 이 책을\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    test_text, \n",
    "    return_tensors = 'pt', \n",
    "    truncation = True, \n",
    "    max_length = max_input_len\n",
    ")\n",
    "inputs.pop('token_type_ids', None)\n",
    "\n",
    "gen_ids = model.generate(\n",
    "    **inputs.to(model.device), \n",
    "    max_new_tokens = 50,            # 출력 토큰의 길이\n",
    "    min_new_tokens = 10,            # 출력 토큰의 최소 길이\n",
    "    num_beams = 4,                  # n개의 후보 문장을 병렬적으로 추적하여 가장 가능성이 높은 문장을 선택\n",
    "    do_sample = False,               # 샘플링의 사용 여부 -> 빔 서치 False , True는 확률에 따라 랜덤 생성\n",
    "    length_penalty = 1.6,           # 생성 길이에 대한 가중치 (반복적으로 요약문이 나오는 경우 해당 값을 조정)\n",
    "    no_repeat_ngram_size = 5,       # 반복 방지\n",
    "    repetition_penalty = 1.5,        # 토큰 반복 패턴이 나타나는 경우 패널티 적용\n",
    "    eos_token_id = tokenizer.eos_token_id, \n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(gen_ids[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
