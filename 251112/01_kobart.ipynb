{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d12652",
   "metadata": {},
   "source": [
    "#### BART\n",
    "- transformer 기반의 모델 \n",
    "- Encoder, Decoder 모두 사용하는 모델 \n",
    "- 장문의 테스트에서 요약 데이터를 생성하는데 사용하는 모델 \n",
    "- Encoder : 입력이 되는 데이터를 BERT형식으로 문장을 이해 ( 순방향, 역방향 )\n",
    "- Decoder : 출력이 되는 요약문은 GPT형식으로 문장을 생성 \n",
    "- 해당 모델에서는 Tokenizer는 Sentencepeice를 이용\n",
    "- 인풋 tokenizer와 아웃풋 toeknizer는 따로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ce8ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a68cff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from datasets import Dataset, DatasetDict\n",
    "# 문장 간의 검증 지표를 만들어주는 라이브러리 \n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, \\\n",
    "    DataCollatorForSeq2Seq, Seq2SeqTrainer, \\\n",
    "    Seq2SeqTrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d58bb2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미리 학습된 모델 선정 (kobart) -> 완벽한 모델 x\n",
    "model_name = \"gogamza/kobart-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c24a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  학습에서 사용할 원문 데이터, 요약 데이터 \n",
    "train_docs = [\n",
    "    '정부는 중소기업 세제 해택과 R&D 세액 공제를 확대한다고 밝혔다', \n",
    "    '해당 기업은 분기 실적에서 매출 성장을 기록했으며 신제품 출시를 예고했다'\n",
    "]\n",
    "train_sums = [\n",
    "    '정부가 중소기업 지원을 확대한다', \n",
    "    '기업이 실적 개선과 신제품 출시를 발표했다'\n",
    "]\n",
    "valid_docs = [\n",
    "    '교육부가 디지털 교과서 도입을 추진한다고 발표했다'\n",
    "]\n",
    "valid_sums = [\n",
    "    '교육부가 디지털 교과서 도입을 추진한다.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374cbcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer 모델에서 list형태의 데이터를 사용x -> Dataset -> DatasetDict\n",
    "raw_ds = DatasetDict(\n",
    "    {\n",
    "        \"train\" : Dataset.from_dict(\n",
    "            {\n",
    "                'document' : train_docs, \n",
    "                'summary' : train_sums\n",
    "            }\n",
    "        ), \n",
    "        'validation' : Dataset.from_dict(\n",
    "            {\n",
    "                'document' : valid_docs, \n",
    "                'summary' : valid_sums\n",
    "            }\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04111af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa2e3da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저, 모델을 로드 \n",
    "# use_fast -> 가속모드 사용 유무 -> kobart 모델에는 RUST기반의 가속모드 존재 \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# 입력 / 출력 문장의 최대 길이 설정 \n",
    "max_input_len = 512\n",
    "max_target_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b9954c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 -> 토큰화 \n",
    "def tok_fn(batch):\n",
    "    # batch -> 인자값 -> 묶음형 데이터셋\n",
    "    # raw_ds의 데이터들의 묶음 -> raw_ds에서 인풋 데이터 -> documet의 데이터\n",
    "    # 입력 데이터의 토큰화 -> 인코딩\n",
    "    inputs = tokenizer(\n",
    "        batch['document'], \n",
    "        max_length = max_input_len, \n",
    "        padding = 'max_length',      # 고정길이 패딩 사용\n",
    "        truncation = True            # 최대 길이보다 긴 경우 자른다.\n",
    "    )\n",
    "    # 출력 데이터의 토큰화 -> 인코딩 \n",
    "    # 아웃풋의 토큰나이저는 아웃풋 전용 토크나이저 사용 \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch['summary'], \n",
    "            max_length = max_target_len, \n",
    "            padding = 'max_length',\n",
    "            truncation = True\n",
    "        )\n",
    "    # inputs에서 사용한 토크나이저와 labels에서 사용한 토크나이저는 다른 토크나이저\n",
    "\n",
    "    # padding 토큰을 -100으로 변경 (CrossEtropyloss에서 -100 이라는 값을 무시)\n",
    "    # tokenizer -> result는 input_ids, attention_mask, token_type_ids 값을 생성\n",
    "    # input_ids : 단어 사전에 있는 인덱스의 값들로 인코딩된 데이터 \n",
    "    # attention_mask : 실제 토큰, 패딩 토큰 \n",
    "    # token_type_ids : 문장의 위치\n",
    "    labels_ids = np.array(labels['input_ids'])\n",
    "    # 행렬에서 pad_token_id와 같은 값을 지닌 데이터를 -100으로 변경\n",
    "    labels_ids[ labels_ids == tokenizer.pad_token_id ] = -100\n",
    "    # labels_ids를 labels에 있는 input_ids에 대입 \n",
    "    inputs['labels'] = labels_ids.tolist()\n",
    "\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d3870c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2 [00:00<?, ? examples/s]c:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 25.10 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 325.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# tok_fn 호출 \n",
    "# raw_ds을 이용하여 tok_fn에 대입 \n",
    "\n",
    "tokenized_ds = raw_ds.map(\n",
    "    tok_fn, \n",
    "    batched= True, \n",
    "    remove_columns=['document', 'summary']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebf147e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb21eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataCollator -> padding 토큰 동적인 처리 + 모델 입력 형태 자동 구성 \n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer= tokenizer, \n",
    "    model = model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42e0f18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 6.27kB [00:00, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "# 검증 지표 선택 \n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b8d05b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    # vocab -> ['A', 'B', 'C']\n",
    "    # 디코더 -> preds = [13, 17, 102, ...] -> \n",
    "    # vocab 인덱스로 구성된 리스트를 다시 문자의 형태로 변환하기 위해서 \n",
    "    # padding 토큰은 무시하기 위해 -100으로 구성 -> 원래의 padding 토큰의 id 값으로 전환\n",
    "    labels = np.where(\n",
    "        labels != -100, labels, tokenizer.pad_token_id\n",
    "    )\n",
    "    # 텍스트 디코딩 \n",
    "    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # 문장에서 좌우의 공백이 존재하는 경우 다른 값으로 측정하기때문에 좌우 공백 제거 \n",
    "    pred_str = [ doc.strip() for doc in pred_str ]\n",
    "    label_str = [ doc.strip() for doc in label_str ]\n",
    "\n",
    "    # ROUGE 계산 \n",
    "    result = rouge.compute(\n",
    "        predictions= pred_str, \n",
    "        references= label_str, \n",
    "        use_stemmer = True       # 점수 계산시 단어의 어간을 기준으로 비교하는 의미\n",
    "    )\n",
    "\n",
    "    # ROUGE를 보기 편한 형태로 변경 \n",
    "    result = { k : round(v * 100, 2) for k, v in result.items() }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60029702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 파라미터 값을 지정 \n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir= \"./kobart\", \n",
    "    eval_strategy= 'epoch', \n",
    "    save_strategy= 'epoch', \n",
    "    learning_rate= 5e-5, \n",
    "    num_train_epochs= 10, \n",
    "    logging_steps=10, \n",
    "\n",
    "    # generate 설정을 변경 \n",
    "    predict_with_generate=True, \n",
    "    generation_max_length= 70, \n",
    "    # 요약 데이터를 생성할때 문장 후보의 탐색의 개수를 설정\n",
    "    generation_num_beams= 4, \n",
    "\n",
    "    load_best_model_at_end= True, \n",
    "    metric_for_best_model='rougeL', \n",
    "    greater_is_better=True, \n",
    "    report_to=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a6ed6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekfla\\AppData\\Local\\Temp\\ipykernel_24768\\780977535.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "c:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.365111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.355832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.281228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.177665</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.384804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.322051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.630048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.963058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.028571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.623100</td>\n",
       "      <td>6.976777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'forced_eos_token_id': 1}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=2.6230655670166017, metrics={'train_runtime': 50.7081, 'train_samples_per_second': 0.394, 'train_steps_per_second': 0.197, 'total_flos': 6097364582400.0, 'train_loss': 2.6230655670166017, 'epoch': 10.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer 생성 \n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model, \n",
    "    args=args, \n",
    "    train_dataset= tokenized_ds['train'], \n",
    "    eval_dataset=tokenized_ds['validation'], \n",
    "    tokenizer = tokenizer, \n",
    "    data_collator= data_collator, \n",
    "    compute_metrics= compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e0b02c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이동통신부는 초거대 AI 연구 인프라 지원을 강화한다고 밝혔다. \n",
      "스타트업 새\n"
     ]
    }
   ],
   "source": [
    "test_text = \"\"\"과학기술정보통신부는 초거대 AI 연구 인프라 지원을 강화한다고 밝혔다. \n",
    "스타트업 새당으로 GPU 리소스를 확대 제공할 계획이다.\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    test_text, \n",
    "    return_tensors = 'pt', \n",
    "    truncation = True, \n",
    "    max_length = max_input_len\n",
    ")\n",
    "inputs.pop('token_type_ids', None)\n",
    "\n",
    "gen_ids = model.generate(\n",
    "    **inputs.to(model.device), \n",
    "    max_length = 20, \n",
    "    num_beams = 4, \n",
    "    do_sample = False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(gen_ids[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
