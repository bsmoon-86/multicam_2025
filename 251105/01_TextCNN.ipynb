{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f984d3be",
   "metadata": {},
   "source": [
    "### 복습 \n",
    "- ratings_train.txt 데이터 로드 \n",
    "- document 컬럼의 결측치 제외\n",
    "- 텍스트 정규화(특수문자 제거, 2칸 이상의 공백 제외, 문자열 좌우 공백 제거)\n",
    "- document의 중복 데이터를 제거 \n",
    "- 데이터프레임에서 상위 1000개만 사용\n",
    "- texts -> 데이터프레임의 document 컬럼의 values\n",
    "- labels -> 데이터프레임의 label 컬럼의 values\n",
    "- 토큰화 -> Komoran \n",
    "    - 선택하는 품사는 NNP NNG VV VA MAG XR\n",
    "    - 불용어 하다, 되다, 이다\n",
    "- 단어 사전 생성 (vocab)\n",
    "    - 단어들 중 최소 출현 횟수가 2회 \n",
    "    - 단어 사전에는 <PAD>, <UNK>을 제일 앞에 지정하여 단어사전 생성\n",
    "- 토큰화 된 문서들을 단어 사전의 위치 값에 맞게 인코딩 (enc_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9521a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re \n",
    "from konlpy.tag import Komoran\n",
    "from collections import Counter\n",
    "# import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ceb65e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150000 entries, 0 to 149999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        150000 non-null  int64 \n",
      " 1   document  149995 non-null  object\n",
      " 2   label     150000 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/ratings_train.txt\", sep = '\\t')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abd39a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document 컬럼의 결측치를 제거하시오\n",
    "df.dropna(subset=\"document\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4916ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "\n",
    "    text = re.sub(r\"[^가-힣0-9a-zA-Z\\s\\.]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document 만 텍스트 정규화 -> Series 데이터에서 normalize() 함수를 실행\n",
    "# 여러 개의 컬럼의 텍스트를 정규화 -> DataFrame 데이터에서 normalize() 함수를 실행\n",
    "\n",
    "\n",
    "# Series 데이터에서 각각의 values들을 normalize() 대입 \n",
    "df['document'].map(normalize)  # 결과를 어디에 대입? -> df['document']\n",
    "# DataFrame 데이터에서 각각의 values들을 normalize() 대입 \n",
    "df[ ['id', 'document'] ] = df[ ['id', 'document'] ].astype(str).applymap(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba4f5639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce17e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document 컬럼의 중복 데이터를 제거 \n",
    "df.drop_duplicates(subset='document', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2dbdf466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.head(1000)\n",
    "df = df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56e4e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 작업 \n",
    "komoran = Komoran()\n",
    "allow_pos = [\"NNP\", \"NNG\", \"VV\", \"VA\", \"MAG\", \"XR\"]\n",
    "stop_word = ['하다', '되다', '이다']\n",
    "\n",
    "def tokenize(doc):\n",
    "    tokens = []\n",
    "    for word, pos in komoran.pos(doc):\n",
    "        if word not in stop_word and pos in allow_pos:\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb93056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts, labels 변수 생성 document, lable 데이터를 대입 \n",
    "texts, labels = df['document'].values, df['label'].values\n",
    "# texts들을 이용하여 토큰화 \n",
    "tokens_list = [ tokenize(doc) for doc in texts ]\n",
    "tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7717648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최소 등장 횟수 2회인 단어사전을 생성 \n",
    "# 토큰화 된 tokens_list에서 단어들의 출현 횟수를 확인 \n",
    "freq = Counter( word for toks in tokens_list for word in toks )\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_list에 있는 token들을 하나의 변수에 대입 \n",
    "t_s = []\n",
    "for toks in tokens_list:\n",
    "    for word in toks:\n",
    "        t_s.append(word)\n",
    "Counter(t_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a139a6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전 생성 \n",
    "min_count = 2\n",
    "\n",
    "# 단어사전 처음에는 특수 토큰 2개를 먼저 대입\n",
    "f_vocab = ['<PAD>', '<UNK>']\n",
    "b_vocab = []\n",
    "for word, cnt in freq.items():\n",
    "    # cnt(출현 횟수)가 min_count(최소 출현 횟수) 보다 크거나 같은 경우 \n",
    "    if cnt >= min_count:\n",
    "        b_vocab.append(word)\n",
    "f_vocab + b_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbe4b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['<PAD>', '<UNK>'] + \\\n",
    "    [ word for word, cnt in freq.items() if cnt >= min_count ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9f6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token화 된 문서들을 인코딩 \n",
    "stoi = {word : idx for idx, word in enumerate(vocab)}\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi2 = dict()\n",
    "for idx, word in enumerate(vocab):\n",
    "    stoi2[word] = idx\n",
    "stoi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c9c10d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead527da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩 함수 정의 \n",
    "def encode(toks):\n",
    "    result = [stoi.get(word, stoi['<UNK>']) for word in toks ]\n",
    "    return result\n",
    "\n",
    "enc_inputs = [ torch.tensor(encode(toks) , dtype=torch.long) \\\n",
    "              for toks in tokens_list ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc5006",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "54187e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 데이터들도 tensor 형태로 변환 \n",
    "labels_t = torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4e3425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터, 검증 데이터 셋트으로 데이터 분할 \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a4e2766",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    enc_inputs, labels_t, test_size=0.2, random_state=42, \n",
    "    stratify= labels_t\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e3fb1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e5dc18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치를 이용해서 1차원 합성곱 학습을 생성 \n",
    "import torch.nn as nn\n",
    "# 데이터셋, 데이터로더 \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# 패딩 토큰 -> 토큰의 길이를 채워주기 위한 기능 \n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8da0999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝에서 사용할 데이터를 파이토치에 맞게 변환 \n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, xs, ys):\n",
    "        # 독립변수를 객체 안에 저장\n",
    "        self.xs = xs\n",
    "        # 종속변수를 객체 안에 저장\n",
    "        self.ys = ys\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.xs[idx], self.ys[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "02205cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 커널의 개수 -> 몇개까지의 단어들을 묶여서 확인할것인가?\n",
    "#                   n-gram의 수\n",
    "MAX_K = 5\n",
    "\n",
    "# collate_fn -> Dataset에서 배치 만큼 데이터를 가져온 뒤 \n",
    "# 데이터의 형태를 변경할때 사용하는 함수\n",
    "def collate_fn(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    pad_id = stoi['<PAD>']  # 0\n",
    "\n",
    "    # batch -> [ [tensor([]), label], [tensor([]), label] ]\n",
    "\n",
    "    # 패딩 토큰 채우기 1차 \n",
    "    # (MAX_K 보다 xs[i] 길이가 작다면 MAX_K 길이로 패딩 토큰을 채워준다)\n",
    "    fixed = []\n",
    "    for x in xs:\n",
    "        if len(x) < MAX_K:\n",
    "            # 필요한 패딩 토큰 개수 : MAX_K - len(x)\n",
    "            need = MAX_K - len(x)\n",
    "            # tensor데이터에 <PAD>를 채워준다.-> 결합 \n",
    "            x = torch.cat(\n",
    "                [\n",
    "                    x,  \n",
    "                    torch.full( (need, ), pad_id, dtype=torch.long )\n",
    "                ]\n",
    "            )\n",
    "        fixed.append(x)\n",
    "    # 패딩 토큰 채우기 2차 \n",
    "    # (xs에서 가장 긴 길이만큼 나머지 xs[i]로 길이를 맞춰서 패딩 토큰 추가)\n",
    "    xs_pad = pad_sequence(\n",
    "        fixed, batch_first=True, padding_value= pad_id\n",
    "    )\n",
    "    lengths = torch.tensor([len(x) for x in fixed], \n",
    "                           dtype=torch.long)\n",
    "    return xs_pad, torch.stack(ys), lengths\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TextDataset(X_train, Y_train), \n",
    "    batch_size= 8, \n",
    "    shuffle = True, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    TextDataset(X_val, Y_val), \n",
    "    batch_size=8, \n",
    "    shuffle=True, \n",
    "    collate_fn= collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a34fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "40b083c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 모델 생성 ( kim CNN )\n",
    "    # Embedding -> Conv1D(k = 3, 4, 5) -> \n",
    "    # max-over-time(해당 구간에서 가장 연관이 높은 구간 선택) -> \n",
    "    # concat (kernel별 높은 구간) -> Dropout -> Linear\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(\n",
    "            self, vocab_size, emb_dim, num_classes, \n",
    "            kernel_size = (3, 4, 5), \n",
    "            num_channel = 100, \n",
    "            pad_idx = 0, \n",
    "            dropout = 0.5\n",
    "    ):\n",
    "        # vocab_size -> 단어 사전의 길이 \n",
    "        # emb_dim -> 임베딩 벡터의 차원의 수 \n",
    "        # num_classes -> 분류 class의 개수\n",
    "        # kernel_size -> 묶이는 단어의 개수 목록\n",
    "        # num_channel -> 합성곱 작업 후 출력의 차원의 개수 \n",
    "        # pad_idx -> 패딩 토큰의 인덱스\n",
    "        # dropout -> 소실되는 데이터의 비율\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, \n",
    "                                emb_dim, padding_idx= pad_idx)\n",
    "        # 1차원 합성곱 모델들\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(in_channels=emb_dim, \n",
    "                          out_channels=num_channel, \n",
    "                          kernel_size= k) for k in kernel_size\n",
    "            ]\n",
    "        )\n",
    "        # 100차원의 모델이 3개가 생성이 되고 출력들을 단순 결합 \n",
    "        # 300차원이 되면서 과적합의 위험\n",
    "        # 일부의 데이터를 소실-> 과적합 방지 \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 선형 모델에서는 분류의 형태로 0, 1의 확률이 출력\n",
    "        self.fc = nn.Linear(num_channel * len(kernel_size), \n",
    "                            num_classes )\n",
    "        # 초기화\n",
    "        self._init_weights()\n",
    "    \n",
    "    # 벡터 초기화하는 함수 \n",
    "    def _init_weights(self):\n",
    "        # 자비에르 초기화\n",
    "        nn.init.xavier_uniform_(self.emb.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        # 합성곱 모델을 초기화 \n",
    "        for conv in self.convs:\n",
    "            # 비선형 구조에서 사용하는 다중 퍼셉트론 Relu()를 이용하는 경우\n",
    "            # 학습이 안정되도록 사용하는 초기화 \n",
    "            nn.init.kaiming_uniform_(conv.weight, a = math.sqrt(5))\n",
    "    \n",
    "    # 순전파\n",
    "    def forward(self, x):\n",
    "        # x -> DataLoader를 통해서 들어오는 입력 데이터 \n",
    "        # batch가 된 데이터들을 collate_fn에 대입하여 나온 결과를 순전파에 입력\n",
    "        x = self.emb(x)\n",
    "        # 배치크기, 시퀀스의 길이 ,임베딩 벡터의 차원의 수 \n",
    "        x = x.transpose(1, 2)\n",
    "        # 배치크기, 임베딩 벡터의 차원수, 스퀀스의 길이\n",
    "\n",
    "        feat_maps = []\n",
    "        # 다중 구조 -> conv -> relu -> Max\n",
    "        for conv in self.convs:\n",
    "            h = torch.relu(conv(x)) # 배치의 크기, relu 차원의 개수, T`: 시퀀스의 길이 - 커널의 사이즈 + 1\n",
    "            # h중 T`의 최대 값\n",
    "            h = torch.max( h, dim = 2 ).values  # 배치의 크기, relu 차원의 개수\n",
    "            feat_maps.append(h)\n",
    "        # 열을 기준으로 feat_maps 단순 결합 \n",
    "        z = torch.cat(feat_maps, dim=1)  # 배치의 크기, relu차원의 개수 * len(self.conv)\n",
    "        # 과적합 방지를 위해 일부 데이터를 0으로 변경 \n",
    "        z = self.dropout(z)\n",
    "        # 선형 모델에서 예측 \n",
    "        logits = self.fc(z)\n",
    "\n",
    "        return logits\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "65ccb7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습의 루프 생성 \n",
    "\n",
    "# 모델 생성 \n",
    "model = TextCNN(\n",
    "    vocab_size= len(vocab), \n",
    "    emb_dim= 128, # 차원의 개수(열의 수)\n",
    "    num_classes = 2, # 분류 class 종류의 수 (0, 1) -> 2\n",
    "    kernel_size= (3, 4, 5), # 몇개의 단어들을 묶을것인가\n",
    "    num_channel= 64, # conv1d에서 output의 차원의 개수\n",
    "    pad_idx= stoi['<PAD>'], # 패딩 토큰의 위치\n",
    "    dropout= 0.5 # 고차원 데이터에서 과적합 방지용 손실 데이터의 비율\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9cf71075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저 설정 \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-3)\n",
    "# 손실 함수 설정 ( 예측값과 실제값을 차이를 계산하는 객체 )\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ec9a7544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습, 예측을 하는 함수 정의 \n",
    "def run_epoch(loader, train = True):\n",
    "    # loader -> model에서 사용할 데이터 \n",
    "    # train -> 학습 모드, 평가 모드 \n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    corret = 0 \n",
    "    total = 0\n",
    "\n",
    "    for x, y, lengths in loader:\n",
    "        # train 매개변수에 따라 자동 미분을 활성화 할것인가\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            # 학습 모드라면 -> 옵티마이저, 백워드 \n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        total_loss += float(loss.item()) * x.size(0)\n",
    "        # 예측값 -> [ 확률, 확률 ] -> 높은 확률의 위치\n",
    "        preds = logits.argmax(dim=1)\n",
    "        corret += int((preds == y).sum().item())\n",
    "        total = x.size(0)\n",
    "    mean_loss = total_loss / total\n",
    "    acc = corret / total\n",
    "    return mean_loss, acc\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
    "    val_loss, val_acc = run_epoch(val_loader, train=False)\n",
    "    print(f\"epoch({epoch}) : train loss({round(tr_loss, 4)}) train acc({round(tr_acc, 4)})\")\n",
    "    print(f\"epoch({epoch}) : val loss({round(val_loss, 4)}) val acc({round(val_acc, 4)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "452907dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값 :  0\n",
      "예측 확률 :  [1.0, 1.0710101555622131e-10]\n"
     ]
    }
   ],
   "source": [
    "# test 예측 함수\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(text):\n",
    "    toks = tokenize(text)\n",
    "    ids = torch.tensor(encode(toks), dtype=torch.long)\n",
    "    pad_id = stoi['<PAD>']\n",
    "\n",
    "    # 모델에서 최대 커널의 크기를 확인 \n",
    "    max_k = max([m.kernel_size[0] for m in model.convs])\n",
    "\n",
    "    if len(ids) < max_k:\n",
    "        need = max_k - len(ids)\n",
    "        ids  = torch.cat(\n",
    "            [\n",
    "                ids, \n",
    "                torch.full(\n",
    "                    (need, ), pad_id, dtype=torch.long\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    x = ids.unsqueeze(0)\n",
    "    logits = model(x)\n",
    "    prob = torch.softmax(logits, dim=1).squeeze(0).tolist()\n",
    "    pred = int(torch.argmax(logits, dim=1).item())\n",
    "    return prob, pred\n",
    "\n",
    "p, yhat = predict( \"직원의 태도가 별로였고 실망했다\" )\n",
    "print(\"예측값 : \", yhat)\n",
    "print(\"예측 확률 : \", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc441eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
