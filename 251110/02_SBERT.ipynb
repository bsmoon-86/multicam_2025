{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12aa0c3a",
   "metadata": {},
   "source": [
    "#### SBERT\n",
    "- BERT 모델 : 문장 이해용 Encoder\n",
    "    - 문장 쌍 비교\n",
    "- SBERT 모델 : 문장 의미 임베딩 \n",
    "    - 벡터의 비교용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a04320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 설치 \n",
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d75b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03db471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 로드 -> 두개의 문장을 비교(코사인 유사도)\n",
    "# 다목적 한국 SBERT\n",
    "model_name = 'jhgan/ko-sroberta-multitask'\n",
    "# 문장 유사도 특화 \n",
    "model_name2 = 'BM-K/KoSimCSE-roberta-multitask'\n",
    "\n",
    "sbert = SentenceTransformer(model_name)\n",
    "sbert2 = SentenceTransformer(model_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1433e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 토큰의 길이를 설정 \n",
    "sbert.max_seq_length = 256\n",
    "sbert2.max_seq_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두개의 문장을 비교 \n",
    "doc1 = \"이 카메라는 색감이 자연스럽고 배터리도 오래간다\"\n",
    "doc2 = \"배터리 성능이 좋고 사진 품질이 뛰어나다\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609188e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두개의 문장을 임베딩 -> 코사인 유사도 계산 \n",
    "# sbert인 경우 \n",
    "with torch.inference_mode():\n",
    "    emb1 = sbert.encode(doc1, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    emb2 = sbert.encode(doc2, convert_to_tensor=True, normalize_embeddings=True)\n",
    "# 코사인 유사도 계산\n",
    "cos_sim = util.cos_sim(emb1, emb2).item()\n",
    "print(\"유사도 : \", round(cos_sim, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6dcc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두개의 문장을 임베딩 -> 코사인 유사도 계산 \n",
    "# sbert2인 경우 \n",
    "with torch.inference_mode():\n",
    "    emb1 = sbert2.encode(doc1, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    emb2 = sbert2.encode(doc2, convert_to_tensor=True, normalize_embeddings=True)\n",
    "# 코사인 유사도 계산\n",
    "cos_sim = util.cos_sim(emb1, emb2).item()\n",
    "print(\"유사도 : \", round(cos_sim, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd0523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db7e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    '삼성전자 주가가 올랐다', \n",
    "    \"코스피가 상승 마감했다\", \n",
    "    '비가 많이 와서 항공편이 취소됬다'\n",
    "]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    embs = sbert2.encode(sentences, convert_to_tensor=True, \n",
    "                         normalize_embeddings=True)\n",
    "\n",
    "# 3개의 문장에서의 유사도를 확인 \n",
    "sim_metrix = util.cos_sim(embs, embs)\n",
    "print(sim_metrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36883511",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = \"증시가 강세였다\"\n",
    "# 임베딩 \n",
    "new_emb = sbert2.encode(new_sentence, convert_to_tensor=True, \n",
    "                        normalize_embeddings=True)\n",
    "# 유사도가 높은 상위의 n개 확인 \n",
    "top_n = 2\n",
    "hits = torch.topk(\n",
    "    util.cos_sim(new_emb, embs).squeeze(0), k = top_n\n",
    ")\n",
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d3e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for score, idx in zip( hits.values.tolist(), hits.indices.tolist() ):\n",
    "    print(f\"{sentences[idx]} | score : {round(score, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3994874c",
   "metadata": {},
   "source": [
    "### 연습 \n",
    "- ratings_train.txt 파일을 로드 \n",
    "- 결측치 제거 \n",
    "- document 컬럼의 문자 정규화(특수문자 제거, 2칸 이상의 공백 제거, 좌우 공백 제거) \n",
    "- 중복 document 제거 , 글자의 수가 1개 이하인 행은 제거  \n",
    "- DataFrame에서 sample(n = 10000, random_state=42)로 임의의 데이터를 추출하여 저장 (head() -> 상위 데이터 | tail() -> 하위 데이터 | sample() -> 무작위 데이터) \n",
    "- train, test 셋으로 8:2 로 데이터분할\n",
    "- sbert 모델은 'BM-K/KoSimCSE-roberta-multitask'을 이용\n",
    "- Dataset을 정의 (Trainer 이용하지 않고 Dataset과 DataLoader 사용)\n",
    "    - `__init__(self, document, labels)`\n",
    "        - 입력받은 document와 labels를 document는 SBERT 모델을 이용하여 인코딩 \n",
    "        - labels 데이터를 tensor형태로 변환 \n",
    "    - `__len__(self)` 함수는 라벨의 길이를 되돌려준다\n",
    "    - `__getitem__(self, idx)` 함수는 인코딩된 데이터[idx], label[idx]를 되돌려준다\n",
    "- Dataset를 train, test를 이용해서 Dataset을 생성 \n",
    "- DataLoarder를 이용하여 배치의 사이즈는 128 shuffle은 True 구성한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4d39531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb33df28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150000 entries, 0 to 149999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        150000 non-null  int64 \n",
      " 1   document  149995 non-null  object\n",
      " 2   label     150000 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드 \n",
    "df = pd.read_csv(\"../data/ratings_train.txt\", sep='\\t')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f62c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치를 제거 \n",
    "df.dropna(subset='document', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7043ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = re.sub(r'[^가-힣0-9a-zA-Z\\s\\.]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "670ccf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['document'] = df['document'].map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11c67680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 데이터를 제거 \n",
    "df.drop_duplicates(subset='document', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d3e1434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144734"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3243a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열의 길이가 1 이하는 제거 -> 1초과인 데이터만 확인 \n",
    "flag = df['document'].str.len() > 1\n",
    "df = df.loc[flag,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29ed8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤한 데이터 10000개추출 sample()\n",
    "df = df.sample(n = 10000, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20c1f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test 분할 (8:2)\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63987ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    4006\n",
       "1    3994\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f42e804a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name BM-K/KoSimCSE-roberta-multitask. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "model_name3 = 'BM-K/KoSimCSE-roberta-multitask'\n",
    "sbert3 = SentenceTransformer(model_name3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6a6def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 정의 \n",
    "class SBERTDataset(Dataset):\n",
    "    # 생성자 함수 -> document, labels 받아와서 document 임베딩, labels는 tensor화\n",
    "    def __init__(self, document, labels):\n",
    "        # no_grad() -> 자동 미분 일시 정지 \n",
    "        # inference_mode() -> 추론 모드 \n",
    "        with torch.inference_mode():\n",
    "            # 로드한 모델을 이용해서 encode 작업 \n",
    "            # convet_to_tensor -> 결과값을 tensor로 받을것인가? (False : list)\n",
    "            # normalize_embeddings -> L2 정규화 할것인가?\n",
    "            self.emb = sbert3.encode(\n",
    "                document, convert_to_tensor = True, normalize_embeddings=True\n",
    "            )\n",
    "        # labels를 tensor화 \n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        # labels의 길이를 되돌려준다.\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.emb[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66efdcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset의 형태로 데이터프레임을 변환 \n",
    "train_ds = SBERTDataset( train_df['document'].tolist(), train_df['label'].tolist() )\n",
    "test_ds = SBERTDataset(test_df['document'].tolist(), test_df['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d2bd8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader를 이용해서 배치 사이즈만큼의 데이터를 생성 \n",
    "train_dl = DataLoader(train_ds, batch_size = 128, shuffle = True)\n",
    "test_dl = DataLoader(test_ds, batch_size = 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b97b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류 모델 정의 ( 입력받은 데이터 -> document tensor, label tensor )\n",
    "class MLPHead(nn.Module):\n",
    "    # 선형 모델에 데이터를 입력하는 형태 \n",
    "    # 선형 모델을 정의할때 인자값 (Linear( 입력데이터의 피쳐의 수, 출력의 피쳐의 수 ))\n",
    "    def __init__(self, input_dim, hidden = 256, num_classes = 2):\n",
    "        super().__init__()\n",
    "        # 다중 퍼셉트론층 구성 \n",
    "        self.net = nn.Sequential(\n",
    "            # 선형 모델 \n",
    "            nn.Linear( input_dim, hidden ), # 입력 768 차원에서 출력은 256 차원\n",
    "            nn.ReLU(),                      # 비선형의 구조를 이해하기 위한 작업\n",
    "            nn.Dropout(0.2),                # 과적합 방지를 위한 소실 작업\n",
    "            nn.Linear( hidden, num_classes )\n",
    "        )\n",
    "    # 순전파 함수 -> 독립변수를 받아서 예측 값을 되돌려준다. \n",
    "    def forward(self, x):\n",
    "        # x : 독립 변수 (document 데이터를 임베딩하고 배치로 묶은 데이터)\n",
    "        result = self.net(x) # 출력이 2차원인 확률 데이터들\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44169838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLPHead class를 생성하기 위해서는 input_dim 매개변수에 인자는 필수 항목\n",
    "# imput_dim -> 입력 데이터(독립변수)의 피쳐의 수를 의미\n",
    "# 입력데이터 -> sbert3모델에서 임베딩이 된 독립변수의 피쳐의 수\n",
    "# sbert3에서 설정이 된 출력 피쳐의 수를 변수에 저장 \n",
    "in_dim = sbert3.get_sentence_embedding_dimension() # 출력 피쳐의 수를 되돌려주는 내장 함수 \n",
    "in_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11dfc96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLPHead 모델을 생성 \n",
    "clf = MLPHead(in_dim)\n",
    "# 손실함수 -> 예측값과 실젯값의 차이를 확인하는 함수 \n",
    "crit = nn.CrossEntropyLoss()\n",
    "# 옵티마이저 \n",
    "opt = torch.optim.AdamW(clf.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e172aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, loss : 0.6295463409423828\n",
      "epoch : 1, loss : 0.4925076322555542\n",
      "epoch : 2, loss : 0.43932614731788633\n",
      "epoch : 3, loss : 0.4204492735862732\n",
      "epoch : 4, loss : 0.4112177505493164\n"
     ]
    }
   ],
   "source": [
    "# 학습 루프 \n",
    "# 학습 모드 전환 \n",
    "clf.train()\n",
    "\n",
    "for epoch in range(5):\n",
    "    total = 0.0\n",
    "    for x, y in train_dl:\n",
    "        # x : document데이터가 임베딩 벡터가 된 묶음(tensor)\n",
    "        # y : labels데이터가 tensor형태 묶음\n",
    "        opt.zero_grad()\n",
    "        # 순전파\n",
    "        logits = clf(x)\n",
    "        # 손실 계산 (예측값, 실젯값)\n",
    "        loss = crit( logits, y )\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        # 스탭 \n",
    "        opt.step()\n",
    "        total += loss.item() * x.size(0)\n",
    "    print(f\"epoch : {epoch}, loss : {total/len(train_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af7fe286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1]\n",
      "[1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터를 이용하여 정확도, f1_score 확인\n",
    "clf.eval()\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for x, y in test_dl:\n",
    "        logits = clf(x)  # 예측 데이터 -> [[ 0.xxx, 0.xxxx ], [], [] , ...]\n",
    "        pred = logits.argmax(dim=1).tolist()    # 예측 데이터 -> [0, 1, 1, 0, ...]\n",
    "        # y_true에 y를 리스트의 형태로 변환하고 데이터를 확장시킨다. \n",
    "        y_true.extend(y.tolist())\n",
    "        y_pred += pred\n",
    "print(y_true)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ee13954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.814\n",
      "f1_score 0.8119312436804853\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score', accuracy_score(y_true, y_pred))\n",
    "print('f1_score', f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c049b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    '와... 영화 진짜 최고였습니다. 또 보고 싶습니다.', \n",
    "    '스토리가 엉망이고 연기도 별로였다. 추천할만한 영화는 아니다', \n",
    "    '그럭저럭 볼만했지만 크게 인상적이진 않았다.'\n",
    "]\n",
    "\n",
    "id2label = {\n",
    "    0 : '부정', \n",
    "    1 : '긍정'\n",
    "}\n",
    "# 예측값을 되돌려주는 함수\n",
    "# model(독립변수)\n",
    "# 독립변수의 튜닝 -> \n",
    "@torch.no_grad()\n",
    "def predict_review(\n",
    "    texts, \n",
    "    batch_size = 128\n",
    "):\n",
    "    # texts : 예측하려고 하는 리뷰의 원문 데이터들\n",
    "    # batch_size : 묶음의 크기 \n",
    "\n",
    "    # texts의 정규화 -> texts(list형태) -> map(), for문을 이용하여 정규화 \n",
    "                #   -> texts(str) -> 1. 문자열을 정규화 함수에 입력, \n",
    "                # 2. 문자열이면 리스트의 형태로 변환\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    # 정규화 함수에 리뷰 데이터를 넣어준다. \n",
    "    texts_norm = [normalize(t) for t in texts]\n",
    "\n",
    "    # 2개의 모델을 평가모드 전환 clf, sbert3\n",
    "    sbert3.eval()\n",
    "    clf.eval()\n",
    "\n",
    "    # 결과 값\n",
    "    result = []\n",
    "\n",
    "    # 배치 데이터로 구성 -> encode -> 분류 모델에 데이터 입력 -> 출력 값을 설정 -> result에 대입\n",
    "    for idx in range(0, len(texts_norm), batch_size):\n",
    "        batch_texts = texts_norm[ idx : idx + batch_size ]\n",
    "        # texts = ['a', b', 'c'] \n",
    "        # batch_size = 2\n",
    "        # 첫번째 반복 구간에서는 \n",
    "        # batch_texts -> ['a', 'b']\n",
    "        # embs -> 벡터화 -> 열의 개수는 sbert3의 아웃풋의 차원의 수(768) \n",
    "        #               -> 행의 개수는 len(batch_texts)\n",
    "        # probs -> [ [ 0.3, 0.7 ] , [ 0.51, 0.49 ]]\n",
    "        # preds -> [ 1 , 0 ]\n",
    "        # sbert3의 encode함수를 이용하여 임베딩 벡터 생성 \n",
    "        embs = sbert3.encode(\n",
    "            batch_texts, \n",
    "            convert_to_tensor=True, \n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        # embs를 clf 모델을 이용하여 예측 확률 데이터를 생성 \n",
    "        logits = clf(embs)\n",
    "        probs = logits.softmax(dim = -1)\n",
    "        preds = probs.argmax(dim=-1).tolist()\n",
    "\n",
    "        for idx2, pred in enumerate(preds):\n",
    "            # 첫번째 반복문의 1번 루프에서 preds -> [1, 0]\n",
    "            # 두번째 반복문의 첫번째 루프 \n",
    "            # idx2 -> 0\n",
    "            # pred -> 1\n",
    "            # prob -> prods[0, 1] -> 0.7\n",
    "            # review -> texts[ 0 + 0 ] -> texts[0]\n",
    "            # idx2 : 인덱스 \n",
    "            # pred : 예측 값(예측 확률의 인덱스 - 확률이 높은 곳의 인덱스(0,1))\n",
    "            # 높은 예측율\n",
    "            prob = float( probs[idx2, pred] )\n",
    "            # 리뷰의 원문 \n",
    "            # 첫번째 반복문의 반복 횟수? -> len(texts) / batch_size + 1\n",
    "            # 두번째 반복문의 반복 횟수? -> \n",
    "            # idx -> 배치의 시작지점\n",
    "            # idx2 -> 시작점부터 얼마만큼 이동했는가?\n",
    "            review = texts[idx + idx2]\n",
    "            # 긍정/부정 라벨링\n",
    "            label = id2label[pred]\n",
    "            # prob, review, label들을 result에 추가 \n",
    "            result.append(\n",
    "                {\n",
    "                    'text' : review, \n",
    "                    'prob' : prob, \n",
    "                    'label' : label\n",
    "                }\n",
    "            )\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbc701ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data = predict_review(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c648e935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '와... 영화 진짜 최고였습니다. 또 보고 싶습니다.',\n",
       "  'prob': 0.9746094942092896,\n",
       "  'label': '긍정'},\n",
       " {'text': '스토리가 엉망이고 연기도 별로였다. 추천할만한 영화는 아니다',\n",
       "  'prob': 0.9662363529205322,\n",
       "  'label': '부정'},\n",
       " {'text': '그럭저럭 볼만했지만 크게 인상적이진 않았다.',\n",
       "  'prob': 0.6602864861488342,\n",
       "  'label': '부정'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b85a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
