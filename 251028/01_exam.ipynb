{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cef65d5",
   "metadata": {},
   "source": [
    "#### 감정 분석 자연어 처리 \n",
    "1. data 폴더 안에 ratings_train.txt 파일을 로드 \n",
    "2. 데이터를 상위 500개 데이터만 추출\n",
    "    - 데이터를 25000부터 30000 번째의 데이터를 이용\n",
    "3. 리뷰 데이터와 감정 데이터로 나눠준다. \n",
    "4. 리뷰 데이터를 토큰화(komoran함수 이용)\n",
    "5. Word2Vec 학습\n",
    "    - window -> 5\n",
    "    - epochs -> 100\n",
    "    - min_count -> 2\n",
    "    - sg -> 1\n",
    "    - seed -> 42\n",
    "6. 벡터화(Word2Vec, 단위 벡터의 평균)\n",
    "7. 분류 모델 ( SVC, Logistic )\n",
    "8. train, test을 이용하여 2개의 모델중 성능이 높은 모델이 무엇인가?\n",
    "9. 단위 백터의 평균의 성능과 단위 벡터 + 중요도 평균의 성능의 차이를 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa55cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e9bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ratings_train.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4835ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 데이터 500개 \n",
    "# df2 = df.head(500)\n",
    "df2 = df.loc[:500, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d3829",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 함수 생성 \n",
    "# komoran 사용 (konlpy 설치가 되어있는 경우)\n",
    "# 설치가 되어있지 않은 경우에는 split()을 이용하여 토큰화 \n",
    "def build_tokenize():\n",
    "    try:\n",
    "        # 라이브러리 로드 -> 라이브러리가 존재하면 코드들 실행 \n",
    "        from konlpy.tag import Komoran\n",
    "        komoran = Komoran()\n",
    "        allow_pos = ['NNP', 'NNG', 'VV', 'VA', 'SL', 'MAG']\n",
    "        def tokenize(text):\n",
    "            tokens = []\n",
    "            for word, pos in komoran.pos(text):\n",
    "                if pos in allow_pos:\n",
    "                    tokens.append(word)\n",
    "            return tokens\n",
    "        # tokenize 함수를 결과로 되돌려준다. \n",
    "        return tokenize\n",
    "    except Exception as e:\n",
    "        print(\"Komoran 사용 불가 : \", e)\n",
    "        return lambda x : x.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bab4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = build_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27cd036",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df2['document'].values\n",
    "Y = df2['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens = [ tokenize(review) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d252b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec을 이용하여 학습(Skip-gram 방식)\n",
    "w2v = Word2Vec(\n",
    "    sentences= X_tokens, \n",
    "    window = 5, \n",
    "    min_count= 2, \n",
    "    sg = 1, \n",
    "    epochs= 100, \n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "wv = w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a028ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_embed_mean(tokens):\n",
    "    vecs = []\n",
    "    for word in tokens:\n",
    "        if word in wv.index_to_key:\n",
    "            vecs.append(wv[word])\n",
    "    result = np.mean(vecs, axis=0) if vecs else np.zeros(wv.vector_size)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f6e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 단어별 중요도 \n",
    "tfidf_vec = TfidfVectorizer(\n",
    "    tokenizer= tokenize, \n",
    "    lowercase= False\n",
    ").fit(reviews)\n",
    "idf = dict(\n",
    "    zip(\n",
    "        # get_feature_names_out() -> Tfidf에서 사용된 단어들의 목록\n",
    "        tfidf_vec.get_feature_names_out(), \n",
    "        # idf_ : 중요도\n",
    "        tfidf_vec.idf_\n",
    "    )\n",
    ")\n",
    "# 단어 별 단위 벡터의 평균과 idf을 곱한다. \n",
    "def sent_embed_tfidf(tokens):\n",
    "    vecs = []\n",
    "    weight = []\n",
    "    for word in tokens:\n",
    "        # tokens에 각각의 단어가 Word2Vec과 TF-IDF에 존재한다면\n",
    "        if word in wv.key_to_index and word in idf:\n",
    "            # vecs -> 단위벡터와 중요도를 곱한 값을 vecs 추가\n",
    "            vecs.append(wv[word] * idf[word])\n",
    "            # weight -> 중요도 데이터를 추가 \n",
    "            weight.append(idf[word])\n",
    "    # vecs의 데이터가 존재하지 않는다면 -> tokens 안에 단어는 존재하지만 Word2Vec이나\n",
    "    # TD-IDF에 단어가 존재하지 않을때\n",
    "    if not vecs:\n",
    "        # 희소 행렬 되돌려준다. 0행렬\n",
    "        result = np.zeros(wv.vector_size)\n",
    "    else:\n",
    "        result = np.sum(vecs, axis=0) / ( np.sum(weight) + 1e-9 )\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaad4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embed = [ sent_embed_mean(token) for token in X_tokens ]\n",
    "X_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9558355",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embed2 = [ sent_embed_tfidf(token) for token in X_tokens ]\n",
    "X_embed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07530a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 2개 객체 생성 \n",
    "svc = SVC(random_state= 42)\n",
    "logi = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b023708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X, Y, model, test_size = 0.2):\n",
    "    # X는 독립 변수\n",
    "    # Y는 종속 변수\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size= test_size, random_state=42, stratify=Y\n",
    "    )\n",
    "    # 모델에 학습\n",
    "    model.fit(X_train, Y_train)\n",
    "    # 학습된 모델에 예측 값\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"정확도 : \", round(\n",
    "        accuracy_score(y_pred, Y_test), 4\n",
    "    ))\n",
    "    print('분류 레포드 : ')\n",
    "    print(classification_report(y_pred, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6dc025",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(X_embed, Y, svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c68dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(X_embed, Y, logi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeecaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(X_embed2, Y, svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea531681",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(X_embed2, Y, logi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1605072f",
   "metadata": {},
   "source": [
    "- df에서 하위 10개 데이터를 이용하여 예측 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44477c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델에 예측의 값을 반환하는 함수 \n",
    "# 세번째 매개변수(vec_type)를 생성 -> 기본값은 'mean'\n",
    "# 'tfidf' 입력이 들어온다면 벡터화 작업은 w2v + ifidf 융합한 벡터화 \n",
    "def predict_sentence_list(sentences, model, vec_type = 'mean'):\n",
    "    # sentences : 문장들의 리스트 \n",
    "    # 문장들을 토큰화 -> 임베딩 \n",
    "    X_test = []\n",
    "    for sent in sentences:\n",
    "        # token() 함수를 호출하여 토큰화 \n",
    "        tokens = tokenize(sent)\n",
    "        # 토큰화 된 문장을 sent_enbed_mean 함수에 입력하여 호출 ( 단위 백터의 평균 )\n",
    "        if vec_type == 'mean':\n",
    "            vec = sent_embed_mean(tokens)\n",
    "        elif vec_type == 'tfidf':\n",
    "            vec = sent_embed_tfidf(tokens)\n",
    "        X_test.append(vec)\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    result = []\n",
    "    for sent, pred in zip(sentences, preds):\n",
    "        label = \"긍정\" if pred == 1 else \"부정\"\n",
    "        result.append([sent, label])\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 -> 예측\n",
    "X_test = df['document'].tail(10).values\n",
    "run_model(X_embed, Y, svc)\n",
    "\n",
    "predict_sentence_list(X_test, svc, vec_type='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e91fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(X_embed2, Y, logi)\n",
    "predict_sentence_list(X_test, logi, vec_type='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ce162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
