{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "560affb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ekfla\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ekfla\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ekfla\\appdata\\roaming\\python\\python311\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ekfla\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ekfla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0a56ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import pandas as pd \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import AutoTokenizer, BertModel, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04d8166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 정규화 함수 \n",
    "def normalize_token_text(text : str) -> str:\n",
    "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s\\.]', \" \", text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1220a250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150000 entries, 0 to 149999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        150000 non-null  int64 \n",
      " 1   document  149995 non-null  object\n",
      " 2   label     150000 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드 \n",
    "df = pd.read_csv(\"../data/ratings_train.txt\", sep='\\t')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8b0b59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 144637 entries, 0 to 144636\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        144637 non-null  int64 \n",
      " 1   document  144637 non-null  object\n",
      " 2   label     144637 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# document의 결측치 제거 \n",
    "df.dropna(subset=['document'], inplace=True)\n",
    "# 텍스트 정규화\n",
    "df['document'] = df['document'].map(normalize_token_text)\n",
    "# document 의 길이가 1이하인 행을 제거 \n",
    "df = df.loc[df['document'].str.len() > 1 ]\n",
    "# 중복데이터 제거\n",
    "df.drop_duplicates(subset=['document'], inplace=True)\n",
    "# 라벨 컬럼의 데이터를 숫자형 데이터로 변환 \n",
    "df['label'] = df['label'].astype(\"int64\")\n",
    "# 데이터프레임의 인덱스를 초기화하고 기존 인덱스는 제거 \n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41681b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트를 위해 일정 구간만 선택 \n",
    "df2 = df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "955f336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame을 Train, Test 셋으로 분할 \n",
    "train_df, test_df =  train_test_split(\n",
    "    df2, test_size= 0.2, random_state= 42, stratify= df2['label']\n",
    ")\n",
    "# BERT 모델에서 사용하는 데이터 타입으로 변경 \n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_ds = Dataset.from_pandas(test_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9e1a826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document', 'label'],\n",
       "    num_rows: 8000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bb750d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 -> AutoTokenizer를 이용하여 BERT 모델에서 사용하는 토큰화 작업을 로드 \n",
    "MODEL_NAME = \"skt/kobert-base-v1\"\n",
    "\n",
    "# use_fast = False -> 기본(파이썬 기반) 토크나이저 \n",
    "    # KoBERT 모델은 sentencepiece 기반 토큰화 -> 기본 모델을 사용을 권장 \n",
    "# use_fast = True -> 빠른 토크나이저 -> Rust, C 기반 \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ef336be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_fn(batch):\n",
    "    # batch?? -> 데이터의 묶음 \n",
    "\n",
    "    # truncation -> 문장이 최대 입력 길이를 초과했을때 자동으로 자를것인가?\n",
    "    # max_length -> 토큰의 최대 길이 -> 128 설정은 BERT 모델의 일반적인 설정\n",
    "    result = tokenizer( batch['document'], truncation= True, max_length=128 )\n",
    "    # result -> input_ids, attention_mask, token_type_ids 등의 포함 \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69f6df1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8000/8000 [00:00<00:00, 13464.78 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 6977.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Dataset 타입의 데이터에서 데이터들의 묶음(batch)를 tok_fn에 대입하여 새로운 Dataset 생성\n",
    "# batched 는 배치들을 병렬 처리할것인가\n",
    "# batch_size -> 하나의 배치의 데이터의 양\n",
    "train_tok = train_ds.map(tok_fn, batched=True, remove_columns=['id', 'document'])\n",
    "test_tok = test_ds.map(tok_fn, batched = False, remove_columns=['id', 'document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b65265e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 8000\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tok\n",
    "# input_ids -> 문장 토큰의 숫자 인덱스(단어사전의 해당 단어의 위치)-> 초기의 단어 사전 인덱스들은 특수 토큰\n",
    "# token_type_ids -> 문장 구분용 인덱스 (0 : 첫번째 문장, 1: 두번째 문장, 2:버그)\n",
    "# attention_mask -> 실제 토큰과 패딩 토큰 분류 (1 : 실제 토큰, 0 : 패딩 토큰) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c55c822e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tok['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "371d28c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 분류 모델을 정의 \n",
    "\n",
    "class BERTClsHead(nn.Module):\n",
    "    def __init__(self, model_name, num_label = 2, dropout = 0.1):\n",
    "        # model_name -> 로드할 모델의 이름 \n",
    "        # num_label -> 분류 클래스의 개수\n",
    "        # dropout -> 데이터의 소실 비율 ( 과적합 방지 )\n",
    "\n",
    "        # 설정 초기화\n",
    "        super().__init__()\n",
    "\n",
    "        # 사전에 학습된 BERT 모델을 로드 (백본)\n",
    "        self.backbone = BertModel.from_pretrained(model_name)\n",
    "\n",
    "        # BERT model에서의 output의 차원 개수 -> 768\n",
    "        hidden = self.backbone.config.hidden_size\n",
    "\n",
    "        # 과적합 방지를 위한 dropout \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # 선형 모델  -> 2개의 class를 분류하는 모델 \n",
    "        self.classifier = nn.Linear(hidden, num_label)\n",
    "\n",
    "        # 패딩 토큰의 아이디 값을 백본 설정에 패딩 아이디에 대입 -> 확인차 대입 (안정성)\n",
    "        self.backbone.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # 초기설정 완료\n",
    "    \n",
    "    # 순전파 함수 생성\n",
    "    def forward(self, input_ids = None, \n",
    "                attention_mask = None, labels = None, **kwargs):\n",
    "        # input_ids -> 토큰화 인코딩 처리가 완료된 문장\n",
    "        # attention_mask -> 실제 단어 / 패딩 단어\n",
    "        # labels -> 학습 시 정답의 라벨 ( 없으면 추론 모드 )\n",
    "\n",
    "        # 백본에 입력 데이터를 대입 \n",
    "        out = self.backbone(input_ids = input_ids, attention_mask = attention_mask)\n",
    "\n",
    "        # [CLS] 토큰 벡터를 추출 \n",
    "        # 입력의 첫번째 토큰 [CLS] -> 문장 전체를 대표하는 의미\n",
    "        pooled = out.last_hidden_state[:, 0]\n",
    "\n",
    "        # 일정 비율 데이터 소실\n",
    "        drop_out_data = self.dropout(pooled)\n",
    "\n",
    "        logits = self.classifier(drop_out_data)\n",
    "\n",
    "        result = {'Logits' : logits}\n",
    "\n",
    "        # labels의 데이터가 존재한다면 손실 계산\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            result['loss'] = loss\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c276b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 생성 \n",
    "model = BERTClsHead(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbe4d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수 정의 (정확도, f1score)\n",
    "def metrics(eval_pred):\n",
    "    # eval_pred -> 예측값, 실젯값\n",
    "    logits, y = eval_pred\n",
    "    # logits [ x.xxx, x.xxx ]\n",
    "    pred = logits.argmax(-1)\n",
    "    return {\n",
    "        'accuracy_score' : accuracy_score(y, pred), \n",
    "        'f1_score' : f1_score(y, pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7e37ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainingArguments -> Trainer가 학습 할때 사용한 각종 설정 값들을 지정하는 객체 \n",
    "\n",
    "args = TrainingArguments(\n",
    "    # 학습된 모델의 결과들을 저장할 디렉토리 지정\n",
    "    output_dir=\"./kobert_from_bertmodel\", \n",
    "    # 배치의 크기를 설정 \n",
    "    per_device_train_batch_size= 16,    # 학습시 cpu/gpu에 할당이 되는 배치의 크기  \n",
    "    per_device_eval_batch_size= 16,     # 평가시 할당이 되는 배치의 크기 \n",
    "    # 평가 및 저장 주기 설정 \n",
    "    eval_strategy= \"epoch\",             # 한 epoch 마다 평가 수행\n",
    "    save_strategy= 'epoch',             # 한 epoch 마다 모델을 저장\n",
    "    # 학습 관련 설정 \n",
    "    num_train_epochs= 2,                # 학습 epoch 수\n",
    "    learning_rate= 5e-5,                # 옵티마이저의 학습율\n",
    "    weight_decay= 0.01,                 # 가중치 감소 계수\n",
    "    warmup_ratio= 0.1,                  # lr의 값을 올리는 비율\n",
    "    logging_steps= 50,                  # 로그를 출력할 step의 간격\n",
    "    # 모델 선택 및 저장 기준 \n",
    "    load_best_model_at_end= True,       # 학습이 끝났을때 가장 성능이 좋은 모델을 자동 로드 \n",
    "    metric_for_best_model= 'f1',        # 최고의 모델을 판단하는 검증 지표\n",
    "    greater_is_better= True,            # 평가 지표가 높을 수록 좋은 모델인가?\n",
    "    # 하드웨어 설정 \n",
    "    fp16= torch.cuda.is_available(),    # cuda사용시 16-bit 혼합정밀도 학습을 할것인가\n",
    "    use_mps_device= (\n",
    "        torch.backends.mps.is_available() if not torch.cuda.is_available() else False\n",
    "    ),                                  # Mac M1/M2 등 Apple silicon 가속기 사용 여부\n",
    "    # 외부 로깅용 설정 \n",
    "    report_to= []\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1cbb326",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekfla\\AppData\\Local\\Temp\\ipykernel_10996\\1394178951.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Trainer -> 모델의 학습을 자동으로 관리하는 Hugging Face의 고수준 API\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,                  # 모델 선택\n",
    "    args = args,                    # 학습에 사용한 설정값\n",
    "    train_dataset= train_tok,       # 학습에 사용할 데이터\n",
    "    eval_dataset= test_tok,         # 평가에 사용할 데이터\n",
    "    tokenizer = tokenizer,          # 토큰화 함수\n",
    "    compute_metrics= metrics        # 평가 시 사용할 검증 지표 함수\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7bd9cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekfla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 01:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평가의 결과 :  {'eval_loss': 0.7059840559959412, 'eval_model_preparation_time': 0.0, 'eval_accuracy_score': 0.506, 'eval_f1_score': 0.09023941068139964, 'eval_runtime': 64.6542, 'eval_samples_per_second': 30.934, 'eval_steps_per_second': 1.933}\n"
     ]
    }
   ],
   "source": [
    "# 평가 및 예측 테스트 \n",
    "\n",
    "eval_res = trainer.evaluate()\n",
    "print(\"평가의 결과 : \", eval_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63d0e895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# 새 문장에 대한 감정 분류 \n",
    "\n",
    "samples = [\n",
    "    '정말 감동적인 영화였습니다', \n",
    "    '지루하고 시간 낭비였습니다'\n",
    "]\n",
    "\n",
    "# 토큰화 \n",
    "enc = tokenizer(\n",
    "    samples, \n",
    "    return_tensors = 'pt',      # pt -> Pytorch 텐서형태로 변환\n",
    "    padding = True, \n",
    "    truncation = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "518ef441",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(**enc)\n",
    "    probs = torch.softmax(\n",
    "        out['Logits'], dim=-1\n",
    "    ).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a30f6e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e003d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정말 감동적인 영화였습니다 : 부정 = 0.585, 긍정 = 0.415 | 예측 = 0\n",
      "지루하고 시간 낭비였습니다 : 부정 = 0.507, 긍정 = 0.493 | 예측 = 0\n"
     ]
    }
   ],
   "source": [
    "for s, p in zip(samples, probs):\n",
    "    print(f\"{s} : 부정 = {p[0]:.3f}, 긍정 = {p[1]:.3f} | 예측 = {p.argmax()}\")\n",
    "    # print(type(p[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
